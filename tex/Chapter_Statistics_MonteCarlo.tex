%Copyright 2016 R.D. Martin
%This book is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
%
%This book is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details, http://www.gnu.org/licenses/.
\chapter{Statistics - The Monte Carlo Method}
\label{Chap:statMonteCarlo}
The Monte Carlo (MC) method was developed to approximate difficult calculations by using random numbers to try and sample the solution. Originally, it was developed at the Los Alamos National Laboratory to simulate the propagation of neutrons in the design of nuclear weapons without having to solve the complex transport equations for neutrons. It has now gained widespread use in a variety of fields beyond physics, including economics, biology, and others. The Monte Carlo method is named after the Monte Carlo casino in Monaco (where James Bond is a regular), because the use of random numbers can remind one of using dice (or other instruments of chance) to perform calculations.

The basic idea behind the Monte Carlo method is to randomly sample the solution space in a clever way that does not require us to calculate the actual distribution of the solutions. For example, you may be interested in the distribution of profits that your poutine restaurant will make based on the number of customers that you get. Since every day you will get a random number of customers, ordering a random number of items, it is very difficult to calculate the distribution of the expected returns per day. With a Monte Carlo simulation, you would generate a random number of customers each day, generate random items to buy, calculate the profit, and then repeat many times. If you know the average distribution of the number of customers and their purchases, your simulation will yield the correct distribution for your expected returns.

\section{Random numbers from a computer}
Monte Carlo simulations require computers to generate random numbers. However, it is not possible to write a computer program that will generate truly random numbers, since computers will always execute a program in a predictable way. For that reason, the random numbers generated by a computer are called ``pseudo random numbers''. Generally, the peudo random number generating (PRNG) algorithms are based on taking a ``seed'' number and then generating a sequence of numbers that is seemingly random (although completely predictable if the seed number and the algorithm are known). 

The sequence will ultimately repeat itself and has a ``period'' that depends both on the seed and the algorithm. A good PRNG will have a large period and will generate random numbers that are ``uniform''. By convention, most random number generators return a floating point value between 0 and 1, that is uniformly distributed between 0 and 1 (that is, no number is more likely than any other). Finally, a good PRNG should produce numbers that are ``uncorrelated'', so that without knowing the algorithm, one would not be able to predict the next number in the sequence. For example, a PRNG could have a large period, produce numbers that are unformly distributed, but alternately produce large and small numbers, so that the sequence is correlated and predictable. If one is relying on a PRNG to produce numbers for a Monte Carlo simulation, and the PRNG is not reliable, the results of the calculation will be incorrect.

Apart from their use in Monte Carlo simulations, PRNGs are used widely in encryption, where very large random numbers are needed to produce encryption keys that are difficult to break. They are also used in a variety of other applications, including electronic lottery machines.

A very simple class of random number generators are the ``Linear Congruential Generators'' (LCG). They are given by the simple recursive relation:
\begin{align*}
 N_i = (aN_{i-1} + b) \; mod \; M 
\end{align*}

where $N_0$ is the seed of the sequence, and ``mod'' is the modulo operation \footnote{The modulo operation gives the remainder of a division. For example, 11 mod 3 is 2; the remainder of dividing 11 by 3 is 2.}. The integer constants $a$, $b$, $M$, determine the properties of the random numbers. $a$ is called the ``multiplier'', $b$ the ``increment'', and $M$ the ``modulus''. In order to obtain numbers, $u_i$, between 0 and 1, one divides $N_i$, by $M$:
\begin{align*}
 u_i = \frac{N_i}{M}
\end{align*}

The period of this class of PRNGs depends on the choice of constants and is at most $M$ ($N_i$ will be between 0 and $M-1$, so there are only $M$ possible values that $N_i$ can take).

A reasonable choice is Park and Miller's ``Minimal Standard'', with $a=16807$, $b=0$, and $M=2^{31}-1=2147483647$, which passes most stringent tests for a PRNG. A particularly terrible choice is the so-called RANDU generator, $a=65539$, $b=0$, $M=2^{31}$. This last choice was implemented on IBM machines for many years and has led to a plethora of inaccurate results (and probably still undiscovered ones).  

The following python code is a simple implementation of the LCG, which uses the Park Miller minimal standard by default. Note the use of a ``function attribute'', \code{LCG.seed}, to store a number that the \code{LCG()} function updates (it starts with the seed of the sequence, and then gets updated with the latest value of $N_i$ to generate a new number the next time that it is called). A histogram of the values generated is shown in Figure \ref{fig:LCG}.
\begin{python}[caption = Park-Miller minimal standard LCG] 
import numpy as np
import pylab as pl

#Define a function that returns LCG (pseudo) random numbers
def LCG (a=16807,b=0,M=2147483647):
    N1 = (a*LCG.seed+b) % M
    LCG.seed = N1
    return N1/M

#Initialize the seed:
LCG.seed=10135

#Generate an array of random numbers from the the LCG
nr=10000
r=np.zeros(nr)
for i in range(nr):
    r[i] = LCG()

#Plot a histogram of the values:
pl.hist(r,bins=100,color='gray')
pl.xlabel('random number')
pl.ylabel('frequency')
pl.title('10,000 numbers for the Park-Miller LCG')
pl.show()
\end{python}
\begin{poutput}
(* \capfig{0.7\textwidth}{figures/LCG.png}{\label{fig:LCG} Distribution of 10,000 numbers generated with the Park Miller Minimal Standard LCG.} *)
\end{poutput}

Although LCG PRNGs are easy to implement, there are many much more advanced algorithms that should be used when one needs random numbers. The LCG algorithm was only given here as an example of ``tricking'' a computer to output seemingly random numbers.

\subsection{Random numbers that follow a given distribution}
Although a good PRNG should be uniform, it is often desirable to generate random numbers according to a specific distribution. For example, one may want to generate numbers that are normally distributed rather than uniformly distributed. This can be done straightforwardly for any distribution using uniformly distributed random numbers.

If we have uniform random numbers, $x$, we wish to find a function, $y(x)$, such that the numbers, $y$, are distributed according to some chosen distribution, $g(y)$. Consider the cumulative distribution function (cdf), G(y), given by:
\begin{align*}
G(y) = \int_{-\infty}^y g(y)dy
\end{align*}

If we now choose the values of $y(x)$ such that $G(y)$ are uniform, then the values of $y$ will be distributed according to $g(y)$. If $x$ is a uniform random number, we can choose the $y$ such that:
\begin{align*}
x(y) = G(y)
\end{align*}
and invert the functions, such that:
\begin{align*}
y(x) = G^{-1}(x)
\end{align*}

The function inversion cannot usually be done analytically, although one can easily program a function inversion using interpolation from tabulated values. Also note that for those distributions, $g(y)$, that are not monotonous, it may not be possible to use this method if the function inverse is not one-to-one. For example, if this method were used for the normal distribution, then it would only generate values on one side of the mean (but the algorithm could easily be modified to generate numbers that are symmetric about the mean).

As an example that can be done analytically, let us suppose that we want to generate random numbers that are distributed according to an exponential probability density function, $g(y) = \frac{1}{\tau}e^{-\frac{y}{\tau}}$. That is, we can generate uniform random numbers, $x$, and we want to know how to transform those numbers, $y(x)$, so that the $y$ are exponentially distributed. The cdf for the exponential distribution, $G(y)$, is easily found:
\begin{align*}
g(y) &= \frac{1}{\tau}e^{-\frac{y}{\tau}}\nonumber\\
G(y) &= \int_{0}^y g(y) = 1- e^{-\frac{y}{\tau}} 
\end{align*}
Setting $G(y)$ equal to $x$ and inverting:
\begin{align*}
x(y) &= G(y)=1- e^{-\frac{y}{\tau}} \\
y(x) &= G^{-1}(x) = -\tau \ln(1-x) 
\end{align*}
Since the $x$ are uniformly distributed between 0 and 1, $1-x$ is also uniformly distributed between 0 and 1, and we can simplify this to:
\begin{align*}
y(x) &= G^{-1}(x) = -\tau \ln{x} 
\end{align*}
where the ``new'' $x$ ($=1-x$) are uniformly distributed between 0 and 1, as were the original $x$. That is, if we generate uniform random numbers, $x$, take their natural logarithm and multiply by $-\tau$, the resulting numbers will have a negative exponential distribution with decay constant $\tau$. This is illustrated in the following python code, which uses the uniform random numbers, \code{r}, generated in the code above with the LCG and converts them to numbers, \code{y}, that are exponentially distributed. The resulting histogram is shown in Figure \ref{fig:LCGexpo}.

\begin{python}[caption = Converting uniform PRNs to exponential PRNs] 
#using the array of values, r, we transform these into values y:
tau = 10
y = -tau * np.log(r) #numpy log is really ln
#Plot a histogram of the values:
n,bins,patches=pl.hist(y,bins=100,color='gray')
pl.xlabel('exponential random number')
pl.ylabel('frequency')
pl.title('10,000 exponentially distributed numbers')
#plot the corresponding exponential (normalized by the number of y values)
xi=np.linspace(0,y.max(),100)
norm = y.size/(bins[1]-bins[0])/tau
pl.plot(xi,norm*np.exp(-xi/tau),color='red',lw=2,label="exponential")
pl.legend()
pl.show()
\end{python}
\begin{poutput}
(* \capfig{0.7\textwidth}{figures/LCGexpo.png}{\label{fig:LCGexpo} Distribution of 10,000 exponentially distributed random numbers obtained from the numbers in Figure \ref{fig:LCG}.} *)
\end{poutput}

In practice, we generally do not need to write our own pseudo-random number generators, and we can usually use built-in functions to generate random numbers according to common distributions. For example, the \code{numpy.random} module in python has many distributions already available, as illustrated in the following code, with histograms of the various random numbers shown in Figure \ref{fig:randomdist}.
\begin{python}[caption = Standard distributions of PRNs] 
nr = 10000 # number of random numbers to generate
#Uniform distribution:
xmin=0
xmax=10
xunif = np.random.uniform(xmin,xmax,nr)
#Normal distribution:
mu = 5
sigma = 2
xnorm = np.random.normal(mu,sigma,nr)
#Binomial distribution:
N=10
p=0.5
xbinom = np.random.binomial(N,p,nr)
#Poisson distribution:
n=3
xpoiss = np.random.poisson(n,nr)
#Plot them all:
bins = np.linspace(0,10,50)
pl.hist(xunif,bins=bins,color='red',alpha=0.5,label='uniform')
pl.hist(xnorm,bins=bins,color='blue',alpha=0.5,label='normal')
pl.hist(xbinom,bins=bins,color='black',alpha=0.5,label='binomial')
pl.hist(xpoiss,bins=bins,color='orange',alpha=0.5,label='poisson')
pl.legend()
pl.show()
\end{python}
\begin{poutput}
(* \capfig{0.7\textwidth}{figures/randomdist.png}{\label{fig:randomdist} Histogram of 10,000 random numbers generated in python with uniform, normal, binomial, and poisson distributions.} *)
\end{poutput}

\section{Monte Carlo simulations}
In this section, we examine a few example of Monte Carlo simulations that use random numbers to model a scenario.
\subsection{Simulating coin tosses}
Let us perform a Monte Carlo simulation of tossing a coin ten times. One ``experiment'' is to throw the coin ten times, and to count the number of heads that we obtained. Over many experiments, we know that the number of heads that we obtain will be binomially distributed. Let us suppose that our coin in unfair and has a probability $p=0.7$ of landing on heads. We can use Monte Carlo simulation to obtain the distribution of the number of times that we get heads in ten coin tosses and, hopefully, recover the binomial distribution.

To simulate a single coin toss, we can generate a random number, $r$, that is uniformly distributed between 0 and 1. We can define the coin toss to be heads if $r<p=0.7$ and tails if $r\geq p= 0.7$. Since $r$ is uniform, on average we will get 70\% heads and 30\% tails, as desired. To simulate counting the number of heads in ten coin tosses (a single experiment), we generate ten values of $r$ and count how many of them are smaller than 0.7, as in the python code below:
\begin{python}[caption = Simulating 10 coin tosses] 
#Simulation of a set of 10 coin tosses:
#Make 10 random numbers between 0 and 1:
N=10
r = np.random.uniform(0.0,1.0,N)
#Count how many are smaller than 0.7
nheads = r[r<0.7].size
print(nheads," heads in 10 tosses")
\end{python}
\begin{poutput}[caption = Note that output is random]
6  heads in 10 tosses
\end{poutput}

We can now simulate this experiment many times (say 10,000), and obtain the distribution of the number of heads that we get, as in the code below:
\begin{python}[caption = Many simulations of throwing a coin 10 times] 
import scipy.stats as stats
Nexp=10000 # repeat the experiment 10,000 times
nheads = np.zeros(Nexp) # array to hold the result of each experiment
N=10
#Conduct the Nexp experiments:
for i in range(Nexp):
    #A single experiment
    r = np.random.uniform(0.0,1.0,N)
    #Count how many are smaller than 0.7, add to array of results
    nheads[i] = r[r<0.7].size    
    
#Plot the result:
bins=np.linspace(-0.5,N+0.5,N+2)
pl.hist(nheads,bins=bins, color='gray')
pl.xlabel("number of heads in 10 tosses")
pl.ylabel("frequency")
#Plot the binomial distribution
norm = Nexp/(bins[1]-bins[0]) #normalization, since the histogram is not normalized
xi=np.arange(N+1)
pl.plot(xi,norm*stats.binom.pmf(xi,N,0.7),color='black',lw=3,label='binomial pmf')
pl.legend(loc='best')
pl.show()
\end{python}
\begin{poutput}
(* \capfig{0.7\textwidth}{figures/coinMC.png}{\label{fig:coinMC} Histogram of the outcome (the number of heads) of 10,000 coin tosses of an unfair coin with $p=0.7$ of landing heads. Overlaid, is the corresponding binomial distribution, which we expect the results to follow.} *)
\end{poutput}
Figure \ref{fig:coinMC} shows a histogram of the number of heads obtained in the 10,000 experiments. The corresponding binomial distribution, with $p=0.7$ and $N=10$ is shown and seen to describe the results of the Monte Carlo simulation, as expected. It is worth noting that we have also effectively implemented an algorithm that can use uniform random numbers to generate binomially-distributed random numbers!



In the previous example, we could have easily calculated the result, since we know from first principles that the distribution of results is binomially distributed. Let us consider a more complicated example, where the distribution of outcomes is not obvious.

\subsection{Simulating a poutine restaurant}
Suppose that we are operating a poutine restaurant, and need to decide if it makes sense to stay open one hour later each day. By using Monte Carlo simulation, we can simulate the expected profits of staying open one extra hour to see if they would on average offset the cost of paying to staff the restaurant. We sell 3 types of poutine (traditional, pork, and vegetarian), and each type has a different popularity and results in different amounts of profit (excluding cost of personnel), as tabulated in Table \ref{tab:poutineProfits}.
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Poutine type} & \textbf{Percent ordered} & \textbf{Profit in \$}\\
\hline
Traditional & 70\% & \$2.00\\
\hline
Pork & 20\% & \$2.50\\
\hline
Vegetarian & 10\% & \$0.50\\
\hline
\end{tabular}
\captionof{table}{\label{tab:poutineProfits} Frequency that each type of poutine is ordered and profit on each type.}
\end{center}

Let us suppose that during a trial period, we have measured that, on average, 15 customers buy poutines in the restaurant during this extra hour. On average, customers buy 1 poutine, but sometimes they buy more (e.g. for their friends). We only count people that enter the store as a customer if they bought at least 1 poutine. For our Monte Carlo simulation, we will consider as a single ``experiment'' determining the profits in one night of staying open for the extra hour. We will then repeat the experiment many times to obtain the distribution of profits that we expect when having the store open for an extra hour for many nights. Our strategy for a single experiment (one night) is as follows:
\begin{enumerate}
\item Draw a random number, \code{ncustomers}, from a Poisson distribution with a mean 15 to be the number of customers that night.
\item For each customer, draw a random number, \code{npoutines}, from a Poisson distribution with a mean of 1 to be the number of poutines ordered by that customer. If the number is 0, draw again. 
\item For each poutine, draw a uniform random number between 0 and 1, \code{choice}, to represent which type of poutine it is. If \code{choice} is between 0 and 0.7, it is a traditional poutine, if it is between 0.7 and 0.9, it is a pork poutine, and if it is between 0.9 and 1, it is a vegetarian poutine.
\item Add up the profits for all of the poutines, for all of the customers.
\end{enumerate}

The following python code performs this Monte Carlo simulation, giving the distribution of expected profits in a histogram shown in Figure \ref{fig:MCpoutine}.

\begin{python}[caption = Monte Carlo of the poutine restaurant] 
Nexp = 10000 # the number of experiments (nights)
profits = np.zeros(Nexp)# array to hold the profits for each night
#Perform Nexp experiments
for iexp in range(Nexp):
    #For each night:
    #Generate a random number of customers with (Poisson) mean 15
    ncustomers = np.random.poisson(15)
    #For each customer, simulate their order:
    for icustomer in range(ncustomers):
        #Generate a random number of poutines with Poisson mean of 1
        #Repeat until the number is bigger than 0
        npoutines=0
        while(npoutines<1):
            npoutines = np.random.poisson(1)
        #For each poutine, decide what type it is
        for ipoutine in range(npoutines):
            #Draw a uniform random number between 0 and 1 for the choice
            choice = np.random.uniform()
            #based on the type, add in the profits for that night:
            if choice<0.7: #traditional
                profits[iexp] = profits[iexp]+2.00 
            elif choice>=0.7 and choice <0.9:  #pork
                profits[iexp] = profits[iexp]+2.50 
            else:  #vegetarian
                profits[iexp] = profits[iexp]+0.50
                
print("Mean profits: ${:.2f}".format(profits.mean())) 
#Plot it:
pl.hist(profits,bins=20,color='gray')
pl.xlabel('profits ($)')
pl.title('Profits simulated for 10,000 nights')
pl.show()
\end{python}
\begin{poutput}
Mean profits: \$46.08
(* \capfig{0.7\textwidth}{figures/MCpoutine.png}{\label{fig:MCpoutine} Histogram of the distribution of profits in the last hour of the poutine store.} *)
\end{poutput}


We can conclude from our simulation that on average, we will generate \$46 of profit by staying open an extra hour, which will be worthwhile if that is enough to cover the cost of the staff. The histogram, if normalized, is truly the probability density function for the expected profits of the store. We can use this pdf to estimate other quantities that may be of interest, such as the profit that we expect to make at lease 90\% of the time (by finding the value where the cumulative distribution function is 0.1). 

\subsection{Monte Carlo simulation for error propagation}
So far, we have used the general formula to propagate uncertainties in measured quantities $x$ and $y$ to the uncertainty in a function $F(x,y)$:
\begin{align}
F&=F(x,y)\nonumber\\
\sigma_{F} &= \sqrt{\left(\die{F}{x}\sigma_x\right)^2 + \left(\die{F}{y} \sigma_y\right)^2+2\die{F}{x}\die{F}{y}\sigma_{xy}}
\end{align}
As you recall from Chapter \ref{chap:StatsNormal}, this formula was derived from a Taylor series expansion of $F(x,y)$ near the best estimates for the measured quantities $x$ and $y$. The Taylor series is only valid as long as the error on $x$ and $y$ are small, which is not necessarily always the case. So how do we handle error propagation when the errors are not small? One obvious, albeit mathematically inconvenient, answer is to add more terms into the Taylor series. We could also use the Min-Max method. However, as we will show here, it turns out the the Monte Carlo method is by far the easiest to apply, and in fact will work in all cases.

Let us suppose that we are interested in measuring Coulomb's force from two charges, $q_1$ and $q_2$, that are a distance $r$ apart. The force is given by:
\begin{align*}
F = k\frac{q_1q_2}{r^2}
\end{align*}
Let us suppose that we have measured all of the quantities with 1\% relative uncertainty, and obtained the following values: $q_1 =10^{-6}$\,C, $q_2 =2\times10^{-5}$\,C, and $r=0.1$\,m (all measurements are $\pm$ 1\%). The following python code calculates the central value and uncertainty in the force using standard error propagation:
\begin{python}[caption = Standard error propagation] 
import numpy as np
import pylab as pl
import sympy as sym

#Measured (or numerical) values:
k_meas = 9e9
e = 1.6e-19
q1_meas = 1e-6
sigma_q1_meas = 0.01*q1_meas
q2_meas = 2e-5
sigma_q2_meas = 0.01*q2_meas
r_meas = 0.1
sigma_r_meas = 0.01*r_meas

#Define symbols
k,q1,q2,r,sigma_q1,sigma_q2,sigma_r= sym.symbols('k, q_1 q_2 r sigma_{q1} sigma_{q2} sigma_{r}')
#F and its uncertainty
F = k*q1*q2/r**2
sigma_F =sym.sqrt((sym.diff(F,q1)*sigma_q1)**2+ (sym.diff(F,q2)*sigma_q2)**2+ (sym.diff(F,r)*sigma_r)**2)
values = [(k,k_meas),(q1,q1_meas),(q2,q2_meas),(r,r_meas),
          (sigma_q1,sigma_q1_meas),(sigma_q2,sigma_q2_meas),(sigma_r,sigma_r_meas)]
F_val = F.subs(values)
sigma_F_val = sigma_F.subs(values)
print("Using error propagation, the force is:")
print("F = {:.2f} +/ {:.2f} N".format(F_val,sigma_F_val))

\end{python}
\begin{poutput}
Using error propagation, the force is:
F = 18.00 +/ 0.44 N
\end{poutput}

Now let us compare this with a Monte Carlo simulation. We will assume that the errors in $q_1$, $q_2$, and $r$ are standard, although it is easy to relax this assumption by using different distributions. We will also assume that the measurements are uncorrelated, as we did above\footnote{Generating correlated random numbers is not too difficult, but it definitely adds a complication!}. The strategy is very simple:
\begin{enumerate}
\item generate $N$ randomly normally distributed\footnote{If the errors were not standard, but corresponded to some different distribution, we could generate the random numbers from that distribution.} samples for $q_1$, $q_2$, and $r$
\item for each sample, calculate a corresponding value of $F$
\item histogram $F$ to see what the distribution looks like
\item usually, pick the mean and the standard deviation of the simulated values of $F$ as the best estimate and uncertainty on $F$
\end{enumerate}
This is accomplished in the following python code:
\begin{python}[caption = Monte Carlo error propagation] 
import numpy as np
import pylab as pl

#Measured (or numerical) values:
k_meas = 9e9
e = 1.6e-19
q1_meas = 1e-6
sigma_q1_meas = 0.01*q1_meas
q2_meas = 2e-5
sigma_q2_meas = 0.01*q2_meas
r_meas = 0.1
sigma_r_meas = 0.01*r_meas

#Generate normally distributed values of r, q1, q2 about their measured values:
N = 1000000 # number of MC samples

#Normally distributed values of q1, q2, and r:
r_sim = np.random.normal(r_meas,sigma_r_meas,N)
q1_sim = np.random.normal(q1_meas,sigma_q1_meas,N)
q2_sim = np.random.normal(q2_meas,sigma_q2_meas,N)

#Generate the corresponding values for the force:
F_sim = k_meas*q1_sim*q2_sim/r_sim**2

#Plot a histogram of the simulated values of F
pl.hist(F_sim,bins=50,color='gray')
pl.xlabel("Simulated Coulomb Force/N")
pl.show()

print("Using Monte Carlo simulation mean and standard deviation, the force is:")
print("F = {:.2f} +/ {:.2f} N".format(F_sim.mean(),F_sim.std(ddof=1)))
\end{python}
\begin{poutput}
Using Monte Carlo simulation mean and standard deviation, the force is:
F = 18.00 +/ 0.44 N
(* \capfig{0.7\textwidth}{figures/MCErrorProp.png}{\label{fig:MCErrorProp} Histogram of the simulated values of the Coulomb force when the relative errors on the measured charges and the separation between them is 1\%.} *)
\end{poutput}


Figure \ref{fig:MCErrorProp} shows the histogram of the simulated values of the Coulomb force, and we note that it is very close to being normally distributed, thus supporting the use of the Taylor series expansion to propagate the uncertainty. When we take the mean and standard deviations of the values in the histogram, we get an estimate of $F$ that is consistent with that obtained from the error propagation formula, as expected. It is worth noting that the Monte Carlo method is quite a bit easier to implement, since it does not require evaluating the partial derivatives.

We now repeat the same exercise, but using 10\% relative uncertainties on all measurements. Modifying the relevant lines in the python code, we get the following output:
\begin{verbatim}
Using error propagtion, the force is:
F = 18.00 +/ 4.41 N
Using Monte Carlo simulation mean and standard deviation, the force is:
F = 18.56 +/ 4.75 N
\end{verbatim}
which now disagree in both the best estimate of $F$ and on the uncertainty. This means that \textbf{the values from the error propagation formula are wrong}. The histogram of the simulated values is shown in Figure \ref{fig:MCErrorProp2}, where we can observe that the distribution of simulated values of $F$ is no longer normally distributed (which we no longer expect if the Taylor series approximation no longer holds).

If we quote our MC estimate of $F$ as the mean and standard deviation of the simulated values, we need to be careful to specify that this is not a standard error, that is, one cannot assume that $F$ is normally distributed about our quoted value. It is ok to use the mean and standard deviation when reporting the result, but one should be very clear in specifying what is reported.

We can also choose to report the estimate of $F$ in a different way, for example, by choosing the mode of the distribution (the ``most probable value'') and choosing an uncertainty that covers 68\% of the area around the mode, as long as we precisely report what was done. The following python code calculates the range of values symmetrically about the mode of the histogram that give approximately 68\% coverage, which is shown by the lines on Figure \ref{fig:MCErrorProp2}.
\begin{python}[caption = Selecting 68\% coverage errors] 
#Histogram the values and keep the counts in each bin
n,bins,patches = pl.hist(F_sim,bins=50,color='gray')
#Find the index of the bin with the highest content (The mode)
n_max =n.max()
index = (np.abs(n-n_max)).argmin()
#The corresponding value of the force
best_F = bins[index]
#Iterate to find the 68% coverage
d=0 # add or subtract d iteratively to index to cover larger ranges of the histogram
area = n[index-d:index+d].sum()/n.sum() #area between index-d and index+d

while area < 0.68: #normlalize area each time it's calculated
    d = d+1
    area = n[index-d:index+d].sum()/n.sum()
 
sigma_F = best_F-bins[index-d]

print("Using Monte Carlo mode and 68% confidence:")
print("The force is {:.2f} +/- {:.2f} N, with {:.1f}% confidence".format(best_F,sigma_F,100*area))

pl.plot([best_F,best_F],[0,n_max],color='red',lw=3,label='mode = {:.2f} N'.format(best_F))
pl.plot([best_F-sigma_F,best_F-sigma_F],[0,n_max],'--',color='red',lw=3,label='lower bound ={:.2f} N'.format(best_F-sigma_F))
pl.plot([best_F+sigma_F,best_F+sigma_F],[0,n_max],'--',color='red',lw=3,label='upper bound ={:.2f} N'.format(best_F+sigma_F))
pl.xlabel("Simulated Coulomb Force/N")
pl.title("F = {:.2f} +/- {:.2f} N, with {:.1f}% confidence".format(best_F,sigma_F,100*area))
pl.legend()
pl.show()
\end{python}
\begin{poutput}
Using Monte Carlo mode and 68% confidence:
The force is 16.21 +/- 4.69 N, with 70.2% confidence
(* \capfig{0.7\textwidth}{figures/MCErrorProp2.png}{\label{fig:MCErrorProp2} Histogram of the simulated values of the Coulomb force when the relative errors on the measured charges and the separation between them is 10\%. The distribution is no longer gaussian. The red lines show the mode of the distribution and a symmetric range about the mode that cover approximately 68\% of the values.} *)
\end{poutput}



It is worth noting that the Monte Carlo method always gives the correct estimate of the best estimate and uncertainty, unlike the error propagation formula which relies on a Taylor series being valid. However, for the Monte Carlo method to be correct, one must also include any potential correlation between the parameters as well as the correct distribution for the measured values (which may not be normally distributed), although this caveat applies to the error propagation formula as well. As a minimum, one should use the Monte Carlo method to evaluate whether the error propagation formula is being used in a valid regime. It is also worth noting that, in general, the Monte Carlo method is easier to implement. It will also likely impress your physics professors if you use the MC method to evaluate the uncertainties in an undergraduate laboratory!




