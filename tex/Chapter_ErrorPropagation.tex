%Copyright 2016 R.D. Martin
%This book is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
%
%This book is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details, http://www.gnu.org/licenses/.
\chapter{Error Propagation}
\label{chap:ErrorPropagation}
In this chapter, we show how to propagate uncertainties from ``direct'' measurements to uncertainties in quantities calculated from those measurements. We will justify many of the rules that we present here in chapter \ref{chap:StatsNormal}.

\section{The Min-Max method}
We will look at the ``Min-Max'' method as an introduction to propagating uncertainties before looking at a more general method that works in all cases. The Min-Max method is based on considering the maximal change in a calculated quantity that can result from the variation of the measured quantities.

\subsection{Summing and subtracting}
First, we consider the case of summing (or subtracting) two numbers with uncertainties. For example, we may be placing two masses ($m_1$, $m_2$) on a balance and need to know their sum, $M=m_1+m_2$. If each individual mass, $m_i$, has an uncertainty of $\sigma_{m_i}$, then the absolute maximal value that the sum of the masses could have is:
\begin{align}
M^{max}\equiv(m_1+\sigma_{m_1})+(m_2+\sigma_{m_2})
\end{align}
which assumes that in each case, the ``true'' value of the masses was at the maximum end of the range defined by our uncertainty. Similarly, if the ``true'' value of the masses was at the minimum end of the range, the minimum value that the sum could be is:
\begin{align}
M^{min}\equiv(m_1-\sigma_{m_1})+(m_2-\sigma_{m_2})
\end{align}
Now that we know the range over which $M$ must lie, we can define a value of $M$ with uncertainty, $\sigma_{M}$, to correspond to the center of the range with an uncertainty that allows the range to be covered:
\begin{align}
M&\equiv\frac{1}{2}(M^{max}+M^{min})\nonumber\\
\sigma_{M}&\equiv\frac{1}{2}(M^{max}-M^{min})
\end{align}
Substituting the expressions for $M^{max}$ and $M^{min}$, we have:
\begin{align}
M&=\frac{1}{2}\left(\left[(m_1+\sigma_{m_1})+(m_2+\sigma_{m_2})\right]+ \left[(m_1-\sigma_{m_1})+(m_2-\sigma_{m_2})\right]\right)\nonumber\\
 &=m_1+m_2\nonumber\\
\sigma_{M}&=\frac{1}{2}\left(\left[(m_1+\sigma_{m_1})+(m_2+\sigma_{m_2})\right]-\left[(m_1-\sigma_{m_1})+(m_2-\sigma_{m_2})\right]\right) \nonumber\\
 &=\sigma_{m_1}+ \sigma_{m_2}
\end{align}
The central value of the calculated quantity, $M$, is thus given by the sum of $m_1$ and $m_2$, and the uncertainty, $\sigma_{M}$, is given by summing the uncertainties $\sigma_{m_1}$ and $\sigma_{m_2}$.

It is easy to show that if there are more than two measurements, say $N$ measurements of quantities $x_i$, each with uncertainty $\sigma_{x_i}$, the best estimate of the sum, $X$, and its uncertainty, $\sigma_X$, are given by:
\begin{align}
X=\sum_{i=1}^{i=N}x_i\nonumber\\
\sigma_X=\sum_{i=1}^{i=N}\sigma_{x_i}
\end{align}

\begin{example}{}{Using the Min-Max method, show that the best estimate and uncertainty on the difference of two measurements, $x_1$ and $x_2$, with uncertainty $\sigma_{x_1}$ and $\sigma_{x_2}$, are given by $X=x_1-x_2$ and $\sigma_{X} =\sigma_{x_2} + \sigma_{x_2}$, respectively.}{} 
The maximum value of the difference is given when $x_1$ has the biggest value in its range, while $x_2$ has the smallest values in its range:
\begin{align*}
X^{max}=(x_1+\sigma_{x_1})-(x_2-\sigma_{x_2})
\end{align*}
and the minimum value that the difference can have is given by:
\begin{align*}
X^{min}=(x_1-\sigma_{x_1})-(x_2+\sigma_{x_2})
\end{align*}
The central value and uncertainties are then:
\begin{align*}
X&=\frac{1}{2}(X^{max}+X^{min})\nonumber\\
 &=\frac{1}{2}\left(\left[(x_1+\sigma_{x_1})-(x_2-\sigma_{x_2})\right]+\left[(x_1-\sigma_{x_1})-(x_2+\sigma_{x_2})\right]\right)\nonumber\\
 &=x_1-x_2\nonumber\\
\sigma_{X} &=\frac{1}{2}(X^{max}-X^{min})\nonumber\\
 &=\frac{1}{2}\left(\left[(x_1+\sigma_{x_1})-(x_2-\sigma_{x_2})\right]-\left[(x_1-\sigma_{x_1})-(x_2+\sigma_{x_2})\right]\right)\nonumber\\
 &=\sigma_{x_1}+ \sigma_{x_2}\nonumber\\
\end{align*}
as required.
\end{example}
We see that for addition and subtraction, the uncertainties on the individual measurements are always summed, whereas the central value is obtained by summing (or subtracting) the individual measurements as if they had no uncertainty. It should be clear that with this method, the final uncertainty is always ``conservative'', that is, it is never going to be too small. In fact, it is almost always too big. In our example with the sum of two masses, if our measurements of $m_1$ and $m_2$ are truly independent and the errors on the individual measurements are truly random errors, it is very unlikely that the sum is actually at one of the extreme ranges given by the uncertainty. On average, it is much more likely for the error in the measurement of one mass to somewhat offset the error in the measurement of the other (unless there is a systematic bias). Our uncertainty estimated this way is almost always an overestimate of the true uncertainty in the sum, and should rather be considered as an upper limit on the uncertainty. 
\subsection{Multiplication and division}
Suppose that we measured the two sides of a rectangle, $l_1$ and $l_2$ with uncertainties $\sigma_{l_1}$ and $\sigma_{l_2}$, and want to know the area of the rectangle, $A=l_1l_2$, and its uncertainty, $\sigma_{A}$. Multiplication (and division) are best handled by using the relative uncertainties, $\frac{\sigma_{l_1}}{l_1}$ and $\frac{\sigma_{l_2}}{l_2}$. We can write the biggest and smallest possible values for, say, $l_1$, in terms of the relative uncertainty as:
\begin{align}
l_1^{max}=l_1\left(1+\frac{\sigma_{l_1}}{l_1}\right)\nonumber\\
l_1^{min}=l_1\left(1-\frac{\sigma_{l_1}}{l_1}\right)\nonumber\\
\end{align}
The biggest possible value of the area of the rectangle is thus:
\begin{align}
A^{max}&=l_1^{max}l_2^{max}\nonumber\\
  &=l_1\left(1+\frac{\sigma_{l_1}}{l_1}\right)l_2\left(1+\frac{\sigma_{l_2}}{l_2}\right)\nonumber\\
  &=l_1l_2\left(1+\frac{\sigma_{l_1}}{l_1}+\frac{\sigma_{l_2}}{l_2}+\frac{\sigma_{l_1}}{l_1}\frac{\sigma_{l_2}}{l_2}\right)\nonumber\\
  &\approx l_1l_2\left(1+\left[\frac{\sigma_{l_1}}{l_1}+\frac{\sigma_{l_2}}{l_2}\right]\right)
\end{align}
where in the last line, we made the approximation that if each relative uncertainty is small, then their product ($\frac{\sigma_{l_1}}{l_1}\frac{\sigma_{l_2}}{l_2}$) is really small and negligible. This is reasonable, for example if each relative uncertainty if of order 10\% (0.1), then their product (0.01) is certainly negligible when compared to their sum (0.2). And often, one would have relative uncertainties well below 10\%.

The minimum value of the area is found similarly:
\begin{align}
A^{min}&=l_1^{min}l_2^{min}\nonumber\\
  &=l_1\left(1-\frac{\sigma_{l_1}}{l_1}\right)l_2\left(1-\frac{\sigma_{l_2}}{l_2}\right)\nonumber\\
  &=l_1l_2\left(1-\frac{\sigma_{l_1}}{l_1}-\frac{\sigma_{l_2}}{l_2}-\frac{\sigma_{l_1}}{l_1}\frac{\sigma_{l_2}}{l_2}\right)\nonumber\\
  &\approx l_1l_2\left(1-\left[\frac{\sigma_{l_1}}{l_1}+\frac{\sigma_{l_2}}{l_2}\right]\right)
\end{align}
The expressions for $A^{min}$ and $A^{max}$ are symmetric about the central value of $l_1l_2$, so we can easily write the value of $A$ and its uncertainty as:
\begin{align}
A \pm \sigma_{A} = l_1l_2\left(1\pm\left[\frac{\sigma_{l_1}}{l_1}+\frac{\sigma_{l_2}}{l_2}\right]\right)
\end{align}
We can thus identify that the central value and uncertainty on $A$ are given by:
\begin{align}
A&=l_1l_2\nonumber\\
\sigma_{A} &= A\left(\frac{\sigma_{l_1}}{l_1}+\frac{\sigma_{l_2}}{l_2}\right)
\end{align}
More conveniently, we can see that the relative uncertainty of $A$ is given by summing the relative uncertainties on $l_1$ and $l_2$:
\begin{align}
\frac{\sigma_{A}}{A}=\frac{\sigma_{l_1}}{l_1}+\frac{\sigma_{l_2}}{l_2}
\end{align}

\begin{example}{}{Use the Min-Max method to determine the uncertainty on the quotient, $Q=\frac{x_1}{x_2}$, of two numbers, $x_1$ and $x_2$, that have uncertainties $\sigma_{x_1}$ and $\sigma_{x_2}$}{}
We proceed in the same way as we did for the product, using the relative uncertainties. The biggest value that the quotient can have is:
\begin{align*}
Q^{max}&=\frac{x_1^{max}}{x_2^{min}}\nonumber\\
  &=\frac{x_1\left(1+\frac{\sigma_{x_1}}{x_1}\right)}{x_2\left(1-\frac{\sigma_{x_2}}{x_2}\right)}
\end{align*}
Again, we have to assume that the relative uncertainties are small, in particular, we make use of the binomial approximation:
\begin{align*}
(1+x)^\alpha\approx 1+\alpha x
\end{align*}
which applies when $x$ is small. In our case:
\begin{align*}
\frac{1}{1-\frac{\sigma_{x_2}}{x_2}}\approx 1+\frac{\sigma_{x_2}}{x_2}
\end{align*}
$Q^{max}$ is thus given by:
\begin{align*}
Q^{max}&=\frac{x_1}{x_2}\left(1+\frac{\sigma_{x_1}}{x_1}\right)\left(1+\frac{\sigma_{x_2}}{x_2}\right)\\
  &\approx \frac{x_1}{x_2}\left(1+\left[\frac{\sigma_{x_1}}{x_1}+\frac{\sigma_{x_2}}{x_2}\right]\right)
\end{align*}
where, again, we assumed that the product of the relative uncertainties was negligible. The minimum value of the quotient is found in a similar way:
\begin{align*}
Q^{min}&=\frac{x_1^{min}}{x_2^{max}}\nonumber\\
  &=\frac{x_1\left(1-\frac{\sigma_{x_1}}{x_1}\right)}{x_2\left(1+\frac{\sigma_{x_2}}{x_2}\right)}\\
  &\approx\frac{x_1}{x_2}\left(1-\frac{\sigma_{x_1}}{x_1}\right)\left(1-\frac{\sigma_{x_2}}{x_2}\right)\\
  &\approx \frac{x_1}{x_2}\left(1-\left[\frac{\sigma_{x_1}}{x_1}+\frac{\sigma_{x_2}}{x_2}\right]\right)
\end{align*}
where we again used the binomial expansion and the fact that the product of the relative uncertainties is negligible. We end up with a very similar result as for multiplication, where the quotient can be written as:
\begin{align*}
Q \pm \sigma Q = \frac{x_1}{x_2}\left(1\pm\left[\frac{\sigma_{x_1}}{x_1}+\frac{\sigma_{x_2}}{x_2}\right]\right)
\end{align*}
and we find that the central value of the quotient is just the quotient of the two numbers without uncertainty and that the relative uncertainty on $Q$ is found by summing the relative uncertainties of the two numbers:
\begin{align*}
Q&=\frac{x_1}{x_2}\\
\frac{\sigma Q}{Q} &= \frac{\sigma_{x_1}}{x_1}+\frac{\sigma_{x_2}}{x_2}
\end{align*}
\end{example}
In summary, for multiplication and division, we sum the relative uncertainties to obtain the relative uncertainty on the result, whereas for summation and subtraction we sum the absolute uncertainties.

We also make the same point for multiplication and division as we did for summation and subtraction: the Min-Max method overestimates the uncertainties. It is very unlikely for the ``true'' values of the measured quantities to be at the extrema of the uncertainty range, and this method really gives an upper limit on the uncertainty of the result.

\subsubsection{Uncertainties in functions}
We can also apply the formalism of the Min-Max method to propagate the uncertainty in a function. If we have a function, $F(x,y)$ of two measured quantities, $x$ and $y$ that have uncertainties $\sigma_{x}$ and $\sigma_{y}$, respectively, it may not be trivial to find the particular combination of $x$ and $y$ that maximize and minimize $F$. If these combinations are found, then $F$ and its uncertainty $\sigma_{F}$, are given by:
\begin{align}
F&\equiv\frac{1}{2}(F^{max}+F^{min})\nonumber\\
\sigma_{F}&\equiv\frac{1}{2}(F^{max}-F^{min})
\end{align}

If $F$ is a monotonic function in both $x$ and $y$, then the situation is relatively straightforward. For example, if $F$ monotonically \textit{increases} as a function of both $x$ and $y$ (e.g. $F(x,y)=\sqrt{x^2+y^2}$), then we have the simple case where:
\begin{align*}
F^{max}=F(x^{max},y^{max})\\
F^{min}=F(x^{min},y^{min})\\
\end{align*}
In more complicated situations, it may be necessary to find out the answer numerically, which thankfully is easy with a computer!

\begin{example}{}{Given the function $F(x,y)=\sin(x+\log y)$, and the measured values of $x=0\pm1$ and $y=0.5\pm0.2$, use the Min-Max method to find the best estimate and the uncertainty in $F$}{}
Since this is not a trivial function, we can solve this numerically using python:

\begin{lstlisting}[frame=single] 
import numpy as np
from math import *

#The function for which we want to find the uncertainties
def F(x,y):
    return np.sin(x+np.log(y))

#x and y, and their uncertainties:
x,sigma_x=0.,1
y,sigma_y=0.5,0.2

#generate 50 values within the range for x and y to scan the values of F
xvalues=np.linspace(x-sigma_x,x+sigma_x,50)
yvalues=np.linspace(y-sigma_y,y+sigma_y,50)

#initialize Fmin and Fmax outside of the possible range (since -1<F<1)
Fmin,Fmax=10,-10

#Now scan all the values of x and y to find the minimum and maximum of F
xmin,xmax,ymin,ymax=0,0,0,0
for xi in xvalues:
    for yi in yvalues:
        f=F(xi,yi)
        if f<Fmin:
            Fmin=f
            xmin=xi
            ymin=yi
        if f>Fmax:
            Fmax=f
            xmax=xi
            ymax=yi
            
print("The range of F is between {0:.2f} and {1:.2f}".format(Fmin,Fmax))
print("F is thus given by: {0:.2f} +/- {1:.2f}".format(0.5*(Fmax+Fmin),0.5*(Fmax-Fmin)))
print("These occurred at x,y = {0:.2f},{1:.2f}, and {2:.2f},{3:.2f}".format(xmin,ymin,xmax,ymax))
\end{lstlisting}
The output is:
\begin{verbatim}
The range of F is between -1.00 and 0.60
F is thus given by: -0.20 +/- 0.80
These occurred at x,y = -0.92,0.52, and 1.00,0.70
\end{verbatim}
\end{example}

\subsubsection{Summary and comments on the Min-Max method}
The Min-Max method can be used to ``propagate'' the uncertainties of measured quantities through summation, subtraction, multiplication and division. In these cases, it is straightforward to apply as the central value of the result is always given by the central values of the measurements without uncertainties (e.g. the central value of a sum of measurements is the sum of the central values of the measurements). \textbf{In the case of summation and subtraction, the uncertainty on the result is given by the sum of the uncertainties on the measurements. In the case of multiplication and division, the relative uncertainty of the result is given by the sum of the relative uncertainties of the measurements}. We can use the Min-Max method to evaluate the uncertainty in a function, but the answer may need to be obtained numerically.

The Min-Max method gives the uncertainty in the result under the ``worst case scenario'', where the true values are assumed to be at the extreme values allowed in the uncertainty ranges, in such a way to make the biggest change in the result. If the errors on the measurements are truly random, it is very unlikely for all of the measurements to be at the extrema of their allowed range, so the \textbf{Min-Max method gives an upper limit for the uncertainty in the result}. 

\section{Adding in quadrature}
It is quite unlikely that the uncertainty in a calculated number is as large as that given by the Min-Max method. For example, in the case of a sum of masses, when we use the Min-Max uncertainty, we are effectively claiming that we think it is quite possible that both measurements were either high or low, which is quite unlikely in reality. This is however not impossible. If there was a bias in the scale that we used to weigh the masses, it is in fact quite likely that both measurements are indeed at one end of the range that we quoted with the uncertainties. If this were the case, then the Min-Max method gives a reasonable uncertainty in the sum of the masses. However, if there was indeed a systematic effect that resulted in the true value being systematically at the higher or lower end of our quoted range, then we should rethink how we determined the uncertainties.

Most often, the uncertainties that we determine have some sort of ``statistical nature'' to them. For example, when we quote a measured values as $a\pm \sigma_a$, we usually mean that $a$ is equally likely to be bigger or smaller than the central value. In fact, when we determine $a$ and $\sigma_a$ as the mean and error on the mean of multiple measurements, we are explicitly assuming that the true value is ``normally distributed'' about the central value. We will define in chapter \ref{chap:StatsNormal} what we really mean by ``normally distributed'', but it is related to saying that there is a 68\% chance that the true value is in the quoted range. It turns out that most measurements are normally distributed, which justifies the following procedure for adding uncertainties (which we prove in Chapter \ref{chap:StatsNormal}).

If the the uncertainties in multiple measurements are normally distributed and are independent of each other, then those uncertainties should be added in quadrature, rather than added. Addition in quadrature is performed by adding the squares of the uncertainties and then taking the square root of the sum. For the case of two masses being added together, $M=m_1+m_2$, the quadrature uncertainty in $M$ is given by:
\begin{align}
\sigma_{M} = \sqrt{\sigma_{m_1}^2+\sigma_{m_2}^2}
\end{align}
This is also true for the case of subtraction. For multiplication and division, instead of adding the relative uncertainties, we add the relative uncertainties in quadrature. For example, if the area of a rectangle is given by $A=l_1l_2$, then the relative uncertainty in $A$ is given by:
\begin{align}
\frac{\sigma_{A}}{A}=\sqrt{\left(\frac{\sigma_{l_1}}{l_1}\right)^2+\left(\frac{\sigma_{l_2}}{l_2}\right)^2}
\end{align}

Addition in quadrature is not limited to only 2 terms; if there are a multiple measurements, all of the uncertainties can be squared and added together before taking the square root.

The uncertainties added in quadrature are always smaller or equal than the uncertainties that are found by straight addition (think of the Pythagorean theorem). Adding in quadrature is only correct if the various measurements are independent of each other. That is, the measurement of one quantity does not affect the measurement of another quantity whose uncertainty is being added in quadrature. If possible, one should check to see if one value is systematically dependent on another, in other words, if one of the measurement is correlated with the other (which we show how to handle in section \ref{sec:corrMeas}).


\section{Uncertainty in a function of one variable}
We now introduce a more general way to propagate uncertainties. We start by considering how an uncertainty, $\sigma_{x}$, in a measurement of $x$ should propagate to an uncertainty $\sigma_{F}$ in a function $F(x)$. We assume that $F(x \pm \sigma_{x})$ is the possible Min-Max range that $F$ could have when $x$ changes by $\pm\sigma_{x}$. This only makes sense if $\sigma_{x}$ is small; for example, if $F=\sin x$ and $\sigma_{x} \approx \pi$, then the maximum change in $F(x)$ most definitely does not occur at $x\pm\sigma_{x}$. However, in the case were $\sigma_{x}$ is small enough, we can always be assured that in that small range, $x\pm\sigma_{x}$,  $F(x)$ is continuous and monotonous (``well behaved''). We can thus use a Taylor series about $F(x)$ to estimate $F(x\pm\sigma_{x})$ (and only keep the first order term):
\begin{align}
F(x\pm\sigma_{x})\approx F(x)\pm\frac{dF}{dx}\sigma_{x}+\dots
\end{align}
Which tells us that $F$ and its uncertainty are given by:
\begin{align}
F \pm \sigma_{F} &= F(x) \pm \frac{dF}{dx}\sigma_{x}\nonumber\\
\therefore \sigma_{F}&=\left|\frac{dF}{dx}\right| \sigma_{x}
\end{align}
That is, the best estimat of $F(x)$ is obtained by evaluating $F(x)$ at the best estimate value for $x$ and the uncertainty, $\sigma_{F}$ is given by the derivative of $F(x)$ with respect to $x$ multiplied by $\sigma_{x}$.  The derivative is the rate of change of $F(x)$ with respect to $x$, so it makes sense that the change in $F(x)$ due to a change in $x$ is given by the derivative.

\begin{example}{}{Show that the uncertainty in $F(x)=ax$, where $a$ is a constant, is given by $\sigma_{F}=a \sigma_{x}$}{}
The derivative of $F(x)$ with respect to $x$ is:
\begin{align*}
\frac{dF}{dx}=a
\end{align*}
Hence the uncertainty is given by:
\begin{align*}
\sigma_{F}&=\frac{dF}{dx} \sigma_{x}=a \sigma_{x}
\end{align*}
as required
\end{example}

\begin{example}{}{Show that the uncertainty in $F(\theta)=\sin\theta$ is given by $\sigma_{F}=\cos\theta\sigma\theta$}{}
The derivative of $F(\theta)$ with respect to $\theta$ is:
\begin{align*}
\frac{dF}{d\theta}=\cos\theta
\end{align*}
Hence the uncertainty is given by:
\begin{align*}
\sigma_{F}&=\left|\frac{dF}{d\theta}\right| \sigma_\theta=\cos\theta\sigma_\theta
\end{align*}
as required. Note that this only works when $\theta$ is expressed in radians!
\end{example}


\section{The general case: functions of multiple variables}

We now consider the case of a function of multiple variables, for example, $F(x,y)$. Again, we assume that the uncertainties in $x$ and $y$, $\sigma_{x}$ and $\sigma_{y}$, are both small, such that $F(x,y)$ is well behaved. That is, the Min-Max range of $F(x,y)$ near the best estimate values of $x$ and $y$ is given by: $F(x\pm\sigma_{x}, y\pm\sigma_{y})$. We again use the Taylor series formula (and limit it to the first order terms), to approximate $F(x,y)$ in the region near the best estimates of $x$ and $y$:
\begin{align}
F(x\pm\sigma_{x}, y\pm\sigma_{y})\approx F(x,y)\pm \left(\die{F}{x}\sigma_{x} + \die{F}{y} \sigma_{y}\right)
\end{align}
where we have now used the partial derivatives\footnote{Recall that when you take the partial derivative with respect to one variable, you treat all the others as constants}, since $F$ depends on multiple variables. The best estimate of $F(x,y)$ is thus found by evaluating $F(x,y)$ at the estimate values of $x$ and $y$. The uncertainty in $F(x,y))$ is given by:
\begin{align}
\label{eqn:maxDerivError}
\sigma_{F} =\left| \die{F}{x} \right| \sigma_{x} +  \left| \die{F}{y} \right| \sigma_{y} \text{      (upper limit)}
\end{align}
This derivation however assumed that both $x$ and $y$ have changed in the ``worst possible way'' so as to lead to the biggest possible change in $F$. It is more likely that the true values of $x$ and $y$ somewhat offset each other in the effect that they have on $F(x,y)$. For example, if $F(x,y)=x+y$, and the true value of $x$ is in the high side of $x\pm \sigma_{x}$ and the true value of $y$ is in the low side of $y \pm \sigma_{y}$, then the true value of $F$ is actually quite close to $x+y$. Again, if the uncertainties in $x$ and $y$ are truly random and independent, then it is quite unlikely that they lead to the maximum possibly uncertainty in $F$. Thus, in the case where the uncertainties are random and uncorrelated, it makes more sense to add them in quadrature:
\begin{align}
\label{eqn:derivPropagate}
\sigma_{F} = \sqrt{\left(\die{F}{x}\sigma_{x}\right)^2 + \left(\die{F}{y} \sigma_{y}\right)^2}\text{      (x and y random and uncorrelated)}
\end{align}
This formula applies no matter how many variables $F$ depends on; you can just add more terms to the sum in quadrature. 

\begin{example}{}{Use the derivative method to find a formula for the uncertainty, $\sigma_{F}$, in $F(x,y)=\frac{x}{y}$ if $x$ and $y$ have uncertainties $\sigma_{x}$ and $\sigma_{y}$. Show that this gives the same uncertainty as found by adding the relative uncertainties in $x$ and $y$ together in quadrature.}{}
We have:
\begin{align*}
F(x,y)&=\frac{x}{y}\\
\therefore \die{F}{x}&=\frac{1}{y}\\
\therefore \die{F}{y}&=\frac{-x}{y^2}
\end{align*}
The uncertainty on $F$ from the derivative method is thus:
\begin{align*}
\sigma_{F}&= \sqrt{\left(\die{F}{x}\sigma_{x}\right)^2 + \left(\die{F}{y} \sigma_{y}\right)^2}\\
  &= \sqrt{\left(\frac{1}{y}\sigma_{x}\right)^2 + \left(\frac{-x}{y^2} \sigma_{y}\right)^2}\\
\end{align*}
If we factor out $\frac{x}{y}$ from the above expression, we have:
\begin{align*}
\sigma_{F}&= \sqrt{\left(\frac{1}{y}\sigma_{x}\right)^2 + \left(\frac{x}{y^2} \sigma_{y}\right)^2}\\
&=\frac{x}{y}\sqrt{\left(\frac{\sigma_{x}}{x}\right)^2 + \left(\frac{\sigma_{y}}{y} \right)^2}
\end{align*}
which is exactly the expression that one obtains from adding the relative uncertainties together. 
\end{example}

\begin{example}{}{Use the derivative method to evaluate a formula for the uncertainty, $\sigma_{F}$, in the function $F(x,y)=\sin(x+\ln y)$ if $x$ and $y$ have uncertainties $\sigma_{x}$ and $\sigma_{y}$. Give the best estimate and uncertainty in $F(x,y)$ for the case of $x=3.0\pm0.5$\,rad and $y=8\pm1$.}{}
We have:
\begin{align*}
F(x,y)&=\sin(x+\log y)\\
\therefore \die{F}{x}&=\cos(x+\ln y)\\
\therefore \die{F}{y}&=\cos(x+\ln y)\frac{d}{dy}x+\log y=\cos(x+\log y)\frac{1}{y}
\end{align*}
The uncertainty in $F$ is thus:
\begin{align*}
\sigma_{F}&= \sqrt{\left(\die{F}{x}\sigma_{x}\right)^2 + \left(\die{F}{y} \sigma_{y}\right)^2}\\
  &= \sqrt{\left(\cos(x+\ln y)\sigma_{x}\right)^2 + \left(\cos(x+\log y)\frac{1}{y} \sigma_{y}\right)^2}\\
\end{align*}
which we can easily do in python:
\begin{lstlisting}[frame=single] 
import sympy as sym
sym.init_printing()
#Let's use sympy to evaluate the error with the derivatives:
#Declare variables x and y and their uncertainties as symbols
x,y,sigma_x, sigma_y = sym.symbols('x y \sigma_x \sigma_y')  
#Build a function out of x and y:
F = sym.sin(x+sym.ln(y)) 
#Find the derivatives
dFdx=sym.diff(F,x)
dFdy=sym.diff(F,y)
#Add them in quadrature to have an expression for sigma_F
sigma_F=sym.sqrt((dFdx*sigma_x)**2+(dFdy*sigma_y)**2)
#Print out the result:
print("Formula for sigma_F from the derivative:\n",sigma_F)
#In a notebook, we can just print it out in pretty version:
sigma_F
\end{lstlisting}
The output is:
\begin{verbatim}
Formula for sigma_F from the derivative:
 sqrt(\sigma_x**2*cos(x + log(y))**2 + \sigma_y**2*cos(x + log(y))**2/y**2)
\end{verbatim}
And then evaluating with actual numbers:
\begin{lstlisting}[frame=single] 
#Let us now evaluate this numerically.
#Put the tuple of values into a list
values=[(x,3.0),(sigma_x,0.5),(y,8.0),(sigma_y,1.0)]
#Create variables that have the variables subbed into the expressions.
#and evaluate those as numbers:
nValue_F=sym.N(F.subs(values))
nValue_sigmaF=sym.N(sigma_F.subs(values))
#Print the result
print("The best estimate and uncertainty in F(x,y) is {:.2f} +/- {:.2f}".format(nValue_F,nValue_sigmaF))
\end{lstlisting}
The output is:
\begin{verbatim}
The best estimate and uncertainty in F(x,y) is -0.93 +/- 0.18
\end{verbatim}
\end{example}

\begin{example}{}{Evaluate the best estimate of $F(x,y,z)$ and its uncertainty, $\sigma_{F}$, for the case where $F(x,y,z)=\sin(x+\ln y)\cos(z)\sqrt{x^2+y^2+2z}$, where the following values have been measured: $x=8\pm0.1$, $y=3\pm0.2$, and $z=10\pm0.3$ }{}
In this case, it is much easier to just use symbolic computation in python.
\begin{lstlisting}[frame=single] 
import sympy as sym
sym.init_printing()
#Let's use sympy to evaluate the error with the derivatives:
#Declare variables x, y and z and their uncertainties as symbols
x,y,sigma_x, sigma_y = sym.symbols('x y \sigma_x \sigma_y')  
z,sigma_z = sym.symbols('z \sigma_z')
#Build a function out of x and y:
F=(sym.sin(x+sym.ln(y))*sym.cos(z))*(sym.sqrt(x**2+y**2+2*z))
#Find the derivatives
dFdx=sym.diff(F,x)
dFdy=sym.diff(F,y)
dFdz=sym.diff(F,z)
#Add them in quadrature to have an expression for sigma_F
sigma_F=sym.sqrt((dFdx*sigma_x)**2+(dFdy*sigma_y)**2+(dFdz*sigma_z)**2)
#The result is a little messy to print out...
#print("Formula for sigma_F from the derivative:\n",sigma_F)
#Create an array of tuples for the numerical values:
values=[(x,8),(sigma_x,0.1),(y,3),(sigma_y,0.2),(z,10),(sigma_z,0.3)]
#Create variables that have the variables subbed into the expressions.
#and evaluate those as numbers:
nValue_F=sym.N(F.subs(values))
nValue_sigmaF=sym.N(sigma_F.subs(values))
#Print the result
print("The best estimate and uncertainty in F(x,y) is {:.2f} +/- {:.2f}".format(nValue_F,nValue_sigmaF))
\end{lstlisting}
The output is:
\begin{verbatim}
The best estimate and uncertainty in F(x,y,z) is -2.59 +/- 1.02
\end{verbatim}
You can easily modify this code to calculate the error on any function. 
\end{example}

\section{Correlated uncertainties}
\label{sec:corrMeas}
We have examined two extreme cases for propagating the uncertainties in measured quantities into the uncertainty for a calculated quantity. In the worst case scenario, when the true values of all of the measurements have conspired to result in the largest change in the computed quantity, the uncertainties should be added together directly (as in equation \ref{eqn:maxDerivError}). In this case, we say that the measurements are ``correlated'', since they have all ``worked together'' to result in the largest possible uncertainty. If the measurements are completely independent from each other, then the uncertainties should be added in quadrature (as in equation \ref{eqn:derivPropagate}).

But what if the measurements are ``somewhat'' correlated? That is, what if measuring one quantity to be high usually makes another quantity high as well, but not always? In the next chapter, we will formalize our definition of the ``covariance'', $\sigma_{xy}$, which is a measure of how correlated two quantities are. If we have two quantities, $x$ and $y$, that we measure simultaneously $N$ times, giving us two sets of measurements  $x_i=\{x_1, x_2,\dots, x_N\}$ and $y_i=\{y_1, y_2,\dots, y_N\}$, the covariance, $\sigma_{xy}$, is defined to be:
\begin{align}
\sigma_{xy}\equiv\frac{1}{N-1}\sum_{i=1}^{i=N}(x_i-\bar x)(y_i-\bar y)
\end{align}
where $\bar x$ and $\bar y$ are the means (as defined back in equation \ref{eqn:MeanAndStd}) of the measurements of $x$ and $y$, respectively.

Suppose that we are interested in the sum of two masses $M=m_1+m_2$, and that we have made several measurements of $m_1$ and $m_2$ which are tabulated in Table \ref{tab:massSum}. For convenience, we have also added a third column with the sum of the masses measured in each row.

\begin{table}[h]
\center
\begin{tabular}{ |c|c|c| }
  \hline
  \textbf{$m_1$ (\,kg)} & \textbf{$m_2$ (\,kg)} & \textbf{$M=m_1+m_2$ (\,kg)}\\
  \hline
  399.3 & 193.2& 592.5 \\ 
  \hline
  404.6 & 205.1& 609.7 \\ 
  \hline
  394.6 & 192.6& 587.2 \\ 
  \hline
  396.3 & 194.2& 590.5 \\ 
  \hline
  399.6 & 196.6& 596.2 \\ 
  \hline
  404.9 & 201.0& 605.9 \\ 
  \hline
  387.4 & 184.7& 572.1 \\ 
  \hline
  404.9 & 215.2& 620.1 \\ 
  \hline
  398.2 & 203.6& 601.8 \\ 
  \hline
  407.2 & 207.8& 615.0 \\ 
  \hline
\end{tabular}
\caption{\label{tab:massSum}Measurements of masses and their sum, for correlated measurements.}
\end{table}
We have determined the mean and the standard deviation (Equation \ref{eqn:MeanAndStd}) for the individual mass measurements to be $\bar m_1 = 399.7$\,kg, $ \sigma_{m1} = 6.01$\,kg, $\bar m_2=199.4$\,kg, and $\sigma_{m2} =  8.87$\,kg (we kept a non-significant figure here to propagate in our uncertainty calculations). So what would should we quote for $M$ and its uncertainty? There are three obvious options:
\begin{enumerate}
\item We can use the mean and error on the mean from the values in the third column. This gives $M=599.1 \pm 4.54$\,kg, which must certainly be correct.
\item We can add the standard deviations of $m_1$ and $m_2$ in quadrature to get the standard deviation of $M$, and then divide by the square root of the number of measurements to get the error on the mean for $M$: $\sigma_M=\frac{1}{\sqrt{N}}\sqrt{\sigma_{m_1}^2+\sigma_{m_2}^2}$. This gives $M= 599.1 \pm 3.39$\,kg, which is an underestimate of the uncertainty.
\item We can add the standard deviations of $m_1$ and $m_2$ (not in quadrature) to get the standard deviation in $M$, and then divide that by the square root of the number of measurements to get the error on the mean for $M$: $\sigma_M=\frac{1}{\sqrt{N}}(\sigma_{m_1}+\sigma_{m_2})$. This gives $M= 599.10 \pm 4.71$\,kg, which is an over estimate.
\end{enumerate}

As expected, the error from the sum in quadrature is smaller than the error from the straight sum. Treating the third column as individual measurements and using the error on the mean from those values must certainly be correct, and should in fact be most representative of the actual variation that we get in the sum. This gives an uncertainty that is larger than the quadrature sum, but not as large as the straight sum. So we have a situation that is closer to the worst case scenario of having to add the uncertainties, but not the worst possible case. We say that the two values of $m_1$ and $m_2$ are ``correlated''. 

We can see this graphically very easily by plotting a scatter plot of $m_1$ and $m_2$, as in Figure \ref{fig:correlatedSum}, where we can see that on average, $m_2$ is higher than its mean (199.4\,kg) when $m_1$ is higher than its mean (399.7\,kg). There is a trend and the values are not randomly distributed. If $m_1$ and $m_2$ really were randomly distributed, we would expect the scatter plot to look like an ellipse. 

\capfig{0.5\textwidth}{figures/correlatedSum.png}{\label{fig:correlatedSum}Two measurements that are not completely independent of each other (correlated).}

The correct way to propagate the uncertainty in quantities that are correlated is to first calculate the covariance factor, $\sigma_{xy}$, and then to use a more general version of equation \ref{eqn:derivPropagate}:
\begin{align}
\label{eqn:derivPropagateCorr}
F&=F(x,y)\nonumber\\
\sigma_F &= \sqrt{\left(\die{F}{x}\sigma_x\right)^2 + \left(\die{F}{y} \sigma_y\right)^2+2\die{F}{x}\die{F}{y}\sigma_{xy}}
\end{align}
where the partial derivatives are evaluated at the central values of $x$ and $y$, and $\sigma_{xy}$ is the covariance between $x$ and $y$. The formula is easily extended for more than 2 variables, where there will be one cross term for each pair of correlated variables. We will derive this formula in Chapter \ref{chap:StatsNormal}.

Using equation \ref{eqn:derivPropagateCorr} for the example of the two masses in table \ref{tab:massSum}, the covariance is determined to be $\sigma_{m_1m_2}=45.61$ leading to an uncertainty in the sum of $\sigma_M=\frac{1}{\sqrt{N}}\sqrt{\sigma_{m_1}^2+\sigma_{m_2}^2+2\sigma_{m_1m_2}}=4.54$\,kg, which is the same result as obtained by treating the third column in Table \ref{tab:massSum} as independent measurements.

We used equation \ref{eqn:derivPropagateCorr} to estimate the standard deviation of the sum, and then divided that by the square root of the number of measurements to obtain the error on the mean of the sum. This is because the covariance is a property that does not depend on the number of measurements, so it cannot be added to the errors on the mean of $m_1$ and $m_2$. We will see in chapter \ref{chap:StatsNormal} that equation \ref{eqn:derivPropagateCorr} comes from estimating the variance (the square of the standard deviation) of the calculated quantity in terms of the variances of the measured quantities (the square root then gives the standard deviation, and dividing by $\sqrt{N}$ then leads to the error on the mean of the calculated quantity).

The code below compares the various computations for estimating the error on the sum of the masses from Table \ref{tab:massSum}:
\begin{lstlisting}[frame=single] 
import numpy as np
from math import *
#Copy and paste the data, add commas by hand
data=np.array([
399.3, 193.2, 592.5,
404.6, 205.1, 609.7,
394.6, 192.6, 587.2,
396.3, 194.2, 590.5,
399.6, 196.6, 596.2,
404.9, 201.0, 605.9,
387.4, 184.7, 572.1,
404.9, 215.2, 620.1,
398.2, 203.6, 601.8,
407.2, 207.8, 615.0])
#Reshape into the correct format
data=data.reshape(10,3)
#Extract the columns:
m1=data[:,0]
m2=data[:,1]
msum=data[:,2]
#Get the mean and standard deviations of the columns:
m1mean=m1.mean() #mean
m1std=m1.std(ddof=1) #std
m2mean=m2.mean() #mean
m2std=m2.std(ddof=1) #std
msmean=msum.mean() #mean
msstd=msum.std(ddof=1) #std
msemean=msstd/sqrt(msum.size) #error on the mean
#Calculate the covariance
cov=((m1-m1mean)*(m2-m2mean)).sum()/(m1.size-1)
#Print out comparisons of the uncertainties
print("m1 = {:.2f} +/- {:.2f}".format(m1mean,m1std))
print("m2 = {:.2f} +/- {:.2f}".format(m2mean,m2std))
print("covariance = {:.2f}".format(cov))
print("Treat as independent measurements: M = {:.2f} +/- {:.2f} (correct)".format(msmean,msemean))
print("Quadrature: M = {:.2f} +/- {:.2f} (underestimate)".format(msmean,sqrt(m1std**2+m2std**2)/sqrt(msum.size)))
print("Sum error: M = {:.2f} +/- {:.2f} (overestimate)".format(msmean,(m1std+m2std)/sqrt(msum.size)))
print("With covariance: M = {:.2f} +/- {:.2f} (correct)".format(msmean,sqrt(m1std**2+m2std**2+2*cov)/sqrt(msum.size)))
\end{lstlisting}
the output is:
\begin{verbatim}
m1 = 399.70 +/- 1.90
m2 = 199.40 +/- 2.81
covariance = 45.67
Treat as independent measurements: M = 599.10 +/- 4.54 (correct)
Quadrature: M = 599.10 +/- 3.39 (underestimate)
Sum error: M = 599.10 +/- 4.71 (overestimate)
With covariance: M = 599.10 +/- 4.54 (correct)
\end{verbatim}

It is worth noting that the covariance, $\sigma_{m_1m_2}$, is negative if the variables are ``anti-correlated''; that is, if one value being predictably higher than its mean leads to the other variable being predictably lower than its mean. A negative covariance can lead to a smaller overall uncertainty in equation \ref{eqn:derivPropagateCorr} than one would get if the variables were independent (by adding in quadrature). Such an example is illustrated by the data in Table \ref{tab:massSum2} and shown in the scatter plot in Figure \ref{fig:correlatedSum2}. The output of the program above, using the data from Table \ref{tab:massSum2} is:
\begin{verbatim}
m1 = 397.60 +/- 3.58
m2 = 196.94 +/- 5.42
covariance = -148.24
Treat as independent measurements: M = 594.55 +/- 3.54 (correct)
Quadrature: M = 594.55 +/- 6.50 (overestimate)
Sum error: M = 594.55 +/- 9.00 (overestimate)
With covariance: M = 594.55 +/- 3.54 (correct)
\end{verbatim}
and shows that the correct uncertainty obtained using the negative covariance is in fact smaller than that obtained by adding in quadrature.

\begin{table}[h!]
\center
\begin{tabular}{ |c|c|c| }
  \hline
  \textbf{$m_1$ (\,kg)} & \textbf{$m_2$ (\,kg)} & \textbf{$M=m_1+m_2$ (\,kg)}\\
  \hline
397.4 & 184.1 & 581.5\\ 
 \hline
388.8 & 205.4 & 594.2\\ 
 \hline
400.5 & 199.4 & 599.9\\ 
 \hline
416.2 & 172.0 & 588.2\\ 
 \hline
396.5 & 212.7 & 609.2\\ 
 \hline
375.2 & 226.4 & 601.6\\ 
 \hline
409.4 & 176.9 & 586.4\\ 
 \hline
400.5 & 209.6 & 610.1\\ 
 \hline
390.0 & 187.0 & 577.0\\ 
 \hline
401.5 & 195.9 & 597.4\\ 
 \hline
\end{tabular}
\caption{\label{tab:massSum2}Measurements of masses and their sum, for anti-correlated measurements.}
\end{table}

\capfig{0.5\textwidth}{figures/correlatedSum2.png}{\label{fig:correlatedSum2}Two measurements that are not completely independent of each other (anti-correlated).}

\clearpage
\section{Averaging numbers with uncertainties}
Suppose that two different experiments have measured the same quantity, $X$, and obtained, say $x_1=10 \pm 0.1$ and $x_2=11 \pm 1$, respectively. We assume that both measurements have correctly estimated their uncertainties, and that these each represent a 68\% confidence of the true value being within their quoted uncertainty range. How do we combine both of these measurements into a single average measurement? How do we compute the uncertainty in the average measurement?

If we use the derivative formula to get the average, we find:
\begin{align*}
X &= \frac{1}{2} (x_1+x_2)=10.5\\
\sigma_{X} &= \frac{1}{2}\sqrt{\sigma_{x_1}^2+\sigma_{x_2}^2}=0.5
\end{align*}
which does not make much sense. The first measurement, $x_1=10 \pm 0.1$, is clearly more precise than the second one, $x_2=11 \pm 1$. Since we trust both measurements, our average must somehow include the fact that $x_1$ is more precise, and we thus expect that it should be closer to 10 than to 11. We also expect that averaging in a less precise result should not ``blow up'' the uncertainty on our number; at worst it should do nothing to the uncertainty (providing the second measurement is consistent with the first one, which it is in this case). In the extreme case, you can imagine that $x_1$ is well measured, and $x_2$ is not measured at all (maybe Team 2 were busy doing other things and forgot to perform the experiment); in this extreme case, a bad measurement of $x_2$ should not impact our knowledge of $X$ or the uncertainty that we obtained from the $x_1$ measurement. If the two measurements are consistent, we expect that the overall uncertainty should decrease. 

The correct way to combine multiple measurements of the same quantity is to perform a weighted average of the measurements, where the uncertainties (squared) are used as the weights. We will justify this approach in chapter \ref{chap:StatsNormal}. If $N$ measurements of $X$, $\{x_1, x_2 ,\dots\}$ each have uncertainties, $\{\sigma_{x_1}, \sigma_{x_2} ,\dots\}$, then the average value is given by:
\begin{align*}
X &= \frac{\sum_{i=1}^{i=N}w_ix_i}{\sum_{i=1}^{i=N}w_i}\\
\sigma_{X} &= \frac{1}{\sqrt{\sum_{i=1}^{i=N}w_i}}\\
w_i&\equiv\frac{1}{\sigma_{x_i}^2}
\end{align*}

If we use this formula for two measurements that we had above, we get:
\begin{align*}
w_1 &= \frac{1}{0.1^2}=100\\
w_2 &= \frac{1}{1^2}=1\\
X &= \frac{\sum_{i=1}^{i=N}w_ix_i}{\sum_{i=1}^{i=N}w_i}=\frac{100\times 10+1\times 11}{101}=10.0099\\
\sigma_{X} &= \frac{1}{\sqrt{\sum_{i=1}^{i=N}w_i}}=\frac{1}{\sqrt{101}}=0.09950\\
\end{align*}
where we kept the extra decimals to show that the uncertainty gets a tiny bit smaller, and the central value of the measurement is shifted very slightly in the direction of $x_2$. As you can see, the first measurement has a weight of 100 and the second measurement only has a weight of 1, in the weighted average. This reflect the factor of 10 difference in the uncertainties from the two measurements, which gets squared when the measurements are combined. 

\section{Summary}
In this chapter, we showed how to propagate the uncertainties from a set of quantities through to results calculated from those quantities. We introduced the Min-Max method as a way to understand the biggest possible change in a calculated quantity resulting from variations in the measured quantities. We also introduced the ``derivative'' method which applies more generally than the Min-Max method. We found that the uncertainty in a calculated quantity is obtained by summing uncertainties in measured quantities. We argued that a quadrature sum of the uncertainties was usually more appropriate in the case where the measured quantities are independent from each other and have random errors. We argued that a straight sum generally provides an upper limit on the uncertainty in a calculated result. The sum in quadrature assumes that the individual quantities are independent from each other (see below for the general case when quantities are allowed to be correlated).

The main formula for propagating uncertainties is given by the derivative method. If a function, $F(x_1, x_2, \dots)$, depends on $N$ independent quantities, $x_1, x_2, \dots$, each with uncertainties $\sigma_{x_1}, \sigma_{x_2}, \dots$, then the best estimate of $F$ is obtained by evaluating $F$ at the best estimate values of the $x_i$, and the uncertainty on $F$, $\sigma_{F}$, is given by the following quadratic sum:
\begin{align}
\sigma_{F} = \sqrt{\sum_{i=1}^{i-N}\left(\die{F}{x_i}\sigma_{x_i}\right)^2}
\end{align}
where $\die{F}{x_i}$ is the partial derivative of $F(x_1, x_2, \dots)$ with respect to $x_i$.

When averaging $n$ measurements $x_i$, each with their own individual uncertainty $\sigma_{x_i}$, the average, $X$, and its uncertainty, $\sigma_{X}$, are given by:
\begin{align*}
X &= \frac{\sum_{i=1}^{i=n}w_ix_i}{\sum_{i=1}^{i=n}w_i}\\
\sigma_{X} &= \frac{1}{\sqrt{\sum_{i=1}^{i=n}w_i}}\\
w_i&\equiv\frac{1}{\sigma_{x_i}^2}
\end{align*}

If we have two quantities, $x$ and $y$, that we measure simultaneously $N$ times, giving us two sets of measurements  $x_i=\{x_1, x_2,\dots, x_N\}$ and $y_i=\{y_1, y_2,\dots, y_N\}$, the covariance between $x$ and $y$, $\sigma_{xy}$, is defined to be:
\begin{align}
\sigma_{xy}\equiv\frac{1}{N-1}\sum_{i=1}^{i=N}(x_i-\bar x)(y_i-\bar y)
\end{align}
where $\bar x$ and $\bar y$ are the means of the measurements of $x$ and $y$, respectively. If the covariance is non-zero, then we need to take it into account and have to modify the formula from the derivative method:
\begin{align}
F&=F(x,y)\nonumber\\
\sigma_{F} &= \sqrt{\left(\die{F}{x}\sigma_x\right)^2 + \left(\die{F}{y} \sigma_y\right)^2+2\die{F}{x}\die{F}{y}\sigma_{xy}}
\end{align}
where $\sigma_x$ and $\sigma_y$ are the standard deviations of the measurements of $x$ and $y$, respectively. If there are more than two variables, one can simply add more terms to the sum (including 1 covariance factor between each pair of variables). This is the most general formula and is always correct. Furthermore, it does not require the individual variables to be normally distributed; this formula gives the correct estimate of the standard deviation in a result, regardless of the distributions for the underlying measurements. In practice, one should estimate whether including the covariance makes a difference, and include it if it is significant.
