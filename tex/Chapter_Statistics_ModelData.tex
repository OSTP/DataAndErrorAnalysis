%Copyright 2016 R.D. Martin
%This book is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
%
%This book is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details, http://www.gnu.org/licenses/.
\chapter{Statistics - Modelling Data}
\label{Chap:statModelData}
In this chapter, we examine the topic of modeling data. We consider separately the aspects of parameter estimation and of model testing. In parameter estimation, we assume that we have a parametric model that can describe our data, and our goal is to determine the parameters of the model. For example, we may expect a linear relationship between two measured quantities and wish to determine the corresponding constant of proportionality (fitting the data with the model). In model testing, we assume that we have a model, and we wish to verify if the data are consistent with the model. For example, we may have data that look like they are linearly related, but we would like to quantify if that is the case. 
 

\section{Fitting a straight line}
\subsection{Linear problems}
It is often possible to model two quantities in an experiment as being linearly related. For example, suppose that we measure the time, $t$, that it takes for objects to fall a certain distance, $x$. From Newton's Second Law, we expect to have:
\begin{align}
x = \frac{1}{2}gt^2
\end{align}
which is not a linear relationship between our measured quantities, $x$ and $t$. However, if every time that we measure $t$, we then take the square of the value, we can turn this into a linear relationship between $x$ and $t^2$, with a slope of $\frac{1}{2}g$. This can be done with a variety of relations that do not appear to be linear at first sight. 

In such an experiment, we may have a series of measurements as in Table \ref{tab:gravmeas}, where we varied the drop height, $x$, from which an object was dropped, and measured the corresponding time, $t$, for it to fall. For each drop height, we measured $t$ multiple times so that we could determine a mean drop time, $\bar t$, with a corresponding standard deviation, $\sigma_t$, and error on the mean, $\sigma_{\bar t}$. We repeated the measurements at each height until we obtained an error on the mean time of 0.1\,s, which we propagated to $t^2$. Let us suppose that we found that the error on the height, $x$, was negligible. 

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{height, $x$ (m)}&\textbf{time, $\bar t$, (s)}&\textbf{$\sigma_{\bar t}$ (s)}&\textbf{$t^2$ (s$^2$)}&\textbf{$\sigma_{t^2}=2t\sigma_{\bar t}$ (s$^2$)}\\
\hline
1.0 & 1.1 & 0.1 & 1.2 & 0.2 \\ 
\hline
2.0 & 1.3 & 0.1 & 1.6 & 0.3 \\ 
\hline
3.0 & 1.3 & 0.1 & 1.7 & 0.3 \\ 
\hline
4.0 & 1.5 & 0.1 & 2.2 & 0.3 \\ 
\hline
5.0 & 1.5 & 0.1 & 2.3 & 0.3 \\ 
\hline
6.0 & 1.6 & 0.1 & 2.4 & 0.3 \\ 
\hline
7.0 & 1.8 & 0.1 & 3.1 & 0.4 \\ 
\hline
8.0 & 1.8 & 0.1 & 3.3 & 0.4 \\ 
\hline
9.0 & 1.8 & 0.1 & 3.1 & 0.4 \\ 
\hline
10.0 & 1.9 & 0.1 & 3.7 & 0.4 \\ 
\hline
\end{tabular}
\captionof{table}{\label{tab:gravmeas} Example measurements of the time, $\bar t$, that it takes to fall a certain distance, $x$.}
\end{center}
Since we varied $x$ while measuring $t$, and we assumed that the uncertainty in $x$ is small, it makes more sense to plot $t^2$ vs $x$, rather that $x$ vs $t^2$, and so we model the data as:
\begin{align}
\label{eqn:t2ofx}
t^2 = \frac{2}{g}x
\end{align}
The data from Table \ref{tab:gravmeas} are plotted in Figure \ref{fig:gravmeas}. The slope of the data in this graph is expected to be $\frac{2}{g}$. We will show in this chapter how to determine the slope and uncertainty on the slope from these data. 

\capfig{0.7\textwidth}{figures/gravmeas.png}{\label{fig:gravmeas} Measured time (squared) to fall a certain distance, using data from Table \ref{tab:gravmeas}.}

\subsection{The Method of Least Squares}
Given a set of $N$ pairs of measurements \{$(x_1,y_1), (x_2,y_2), \dots, (x_N,y_N)$\}, we wish to determine the best estimates (and uncertainties) for the slope, $m$, and offset, $b$, of a straight line:
\begin{align}
\label{eqn:mbmodel}
y=mx+b
\end{align}
that best match the data. For simplicity, we assume that $x$ can be measured with negligible uncertainty compared to the uncertainties on $y$. Furthermore, we assume that each value of $y$, $y_i$, have an uncertainty, $\sigma_{yi}$.

Equation \ref{eqn:mbmodel} is our model for the data. At a given value of $x=x_i$, we expect that we should measure $y^{exp}(x_i)=mx_i+b$. However, since we cannot measure $y$ exactly (due to random errors in our measurement), we will measure a value $y_i$ that is close to the expected value. If the errors in our measurement are truly random, and we measure $y$ many times for a fixed value of $x=x_i$, then we expect a normal distribution of $y_i$ with a mean of $y^{exp}(x_i)$ and a standard deviation, $\sigma_{yi}$. Given a value of $x=x_i$ and our model, the probability\footnote{Although this is technically not a probability (it is a probability density), the ultimate goal will be to maximize the probability, which is also maximized when the probability density is maximized.} of measuring a specific value, $y_i$, is given by the normal distribution:
\begin{align}
P(x_i,y_i)&=\frac{1}{\sigma_{yi}\sqrt{2\pi}}e^{-\frac{(y_i-y^{exp}(x_i))^2}{2\sigma_{yi}^2}}
\end{align}
where the mean of the distribution is given by $y^{exp}(x_i)=mx_i+b$ for the model in equation \ref{eqn:mbmodel}.

In order to estimate the parameters of our model, $m$ and $b$, we use the principle of maximum likelihood to find the value of $m$ and $b$ that yield the highest joint probability (i.e. likelihood) of obtaining our set of measurements. The joint probability of obtaining our data set, given values of $m$ and $b$, is given by:
\begin{align*}
L(x,y|m,b) = P(x_1,x_2,\dots,x_N,y_1,y_2,\dots,y_N|m,b)\propto\prod_{i=1}^{i=N}e^{-\frac{(y_i-y^{exp}(x_i))^2}{2\sigma_{yi}^2}}
\end{align*}
where we have ignored the normalization constant $\frac{1}{\sigma_{yi}\sqrt{2\pi}}$ since it does not depend on $m$ and $b$ and we are only interested in the maximal value in terms of $m$ and $b$. As we did in the previous chapter, we can take the negative of the logarithm of the likelihood (which we will then want to minimize):
\begin{align}
-\ln{L(x,y|m,b)}&=\frac{1}{2}\sum_{i=1}^{i=N}\frac{(y_i-y^{exp}(x_i))^2}{\sigma_{yi}^2}
\end{align}
We can focus on minimizing the chi-squared, $\chi^2$, given by:
\begin{align}
\chi^2=\sum_{i=1}^{i=N}\frac{(y_i-y^{exp}(x_i))^2}{\sigma_{yi}^2}
\end{align}
The chi-squared is related to the original likelihood by:
\begin{align}
\label{eqn:chilike}
L(x,y|m,b) \propto e^{-\frac{\chi^2}{2}}
\end{align}

Before proceeding, we can pause to think of what it means to minimize $\chi^2$. The formula for chi-squared is a sum over all data points of the square distance between the point, $y_i$, and the model prediction, $y^{exp}(x_i)$. That squared distance is then divided (or scaled) by the uncertainty on the point. Thus, if the points are on average 1 standard deviation away from the model prediction, then they will add approximately 1 unit to the chi-squared. For a good model, we expect the chi-squared to be about equal to the number of points. It should be apparent why the method is called the ``method of least squares'', since we are effectively minimizing the squared distance between the model and the data when minimizing the chi-squared.

We assume that each point $y_i$ have the same uncertainty, $ \sigma_{yi} = \sigma_y$. Inserting our model explicitly for $y^{exp}(x_i)$:
\begin{align}
\chi^2=\sum_{i=1}^{i=N}\frac{(y_i-mx_i-b)^2}{\sigma_{y}^2}
\end{align}
By taking the derivative of $\chi^2$ with respect to $m$ and $b$, we can find the values that lead to a minimum:
\begin{align}
\frac{d\chi^2}{dm}&=\frac{d}{dm}\sum_{i=1}^{i=N}\frac{(y_i-mx_i-b)^2}{\sigma_{y}^2}=\frac{-2}{\sigma_y^2}\sum_{i=1}^{i=N}x_i(y_i-mx_i-b)\\
\frac{d\chi^2}{db}&=\frac{d}{db}\sum_{i=1}^{i=N}\frac{(y_i-mx_i-b)^2}{\sigma_{y}^2}=\frac{-2}{\sigma_y^2}\sum_{i=1}^{i=N}(y_i-mx_i-b)
\end{align}
Setting these two equations equal to zero:
\begin{align}
\sum_{i=1}^{i=N}x_i(y_i-mx_i-b)=\sum_{i=1}^{i=N}x_iy_i-m\sum_{i=1}^{i=N}x_i^2-b\sum_{i=1}^{i=N}x_i&=0\\
\sum_{i=1}^{i=N}(y_i-mx_i-b)=\sum_{i=1}^{i=N}y_i-m\sum_{i=1}^{i=N}x_i-Nb&=0
\end{align}
and rearranging:
\begin{align}
m\sum_{i=1}^{i=N}x_i^2+b\sum_{i=1}^{i=N}x_i&=\sum_{i=1}^{i=N}x_iy_i\\
m\sum_{i=1}^{i=N}x_i+Nb&=\sum_{i=1}^{i=N}y_i
\end{align}
which is a system of linear equations that is easily solved for $m$ and $b$:
\begin{align}
\label{eqn:LSmb}
m=\frac{N\sum xy-\sum x\sum y}{N\sum x^2-\left(\sum x\right)^2}\nonumber\\
b=\frac{\sum x^2\sum y-\sum x \sum xy}{N\sum x^2-\left(\sum x\right)^2}
\end{align}
where we have omitted the indices on the $x_i$ and $y_i$ and we have implied that the sums are from $i=1$ to $i=N$. Now that we have formulas for the $m$ and $b$, we can propagate the errors to obtain:
\begin{align}
\label{eqn:LSmberror}
\sigma_m&=\sigma_y\sqrt{\frac{N}{N\sum x^2-\left(\sum x\right)^2}}\nonumber\\
\sigma_b&=\sigma_y\sqrt{\frac{\sum x^2}{N\sum x^2-\left(\sum x\right)^2}}
\end{align}

\begin{example}{Use equations \ref{eqn:LSmb} and \ref{eqn:LSmberror} to determine the slope and offset for a line that best fits the data in Figure \ref{fig:gravmeas}, and thus determine a value of the gravitational constant $g$ with its uncertainty from the data.}
\label{ex:gravLMfit}
We want to plot the data of $t^2$ as a function of $x$, and determine the slope and offset of those data. The slope will be equal to $\frac{2}{g}$ (equation \ref{eqn:t2ofx}), and can thus be used to determine our measured value of $g$. We can use equations \ref{eqn:LSmb} and \ref{eqn:LSmberror} to determine the slope and offset, and then use error propagation to convert the error in the slope into an error in $g$. If the error in the slope is $\sigma_m$, the error in $g$, $\sigma_g$, will be given by:
\begin{align*}
g&=\frac{2}{m}\\
\sigma_g&=\sqrt{\left(\frac{dg}{dm}\sigma_m\right)^2}=\frac{2}{m^2}\sigma_m
\end{align*}
The rest of the calculations are done easily in python:
\begin{python}[caption = Linear fit with least squares] 
import numpy as np
from math import *
import pylab as pl
import scipy.stats as stats
%matplotlib inline

#Copy and paste data from the table in the notes, add commas:
datalist=[
1.0, 1.1, 0.1, 1.2, 0.2,
2.0, 1.3, 0.1, 1.6, 0.3,
3.0, 1.3, 0.1, 1.7, 0.3,
4.0, 1.5, 0.1, 2.2, 0.3,
5.0, 1.5, 0.1, 2.3, 0.3,
6.0, 1.6, 0.1, 2.4, 0.3,
7.0, 1.8, 0.1, 3.1, 0.4,
8.0, 1.8, 0.1, 3.3, 0.4,
9.0, 1.8, 0.1, 3.1, 0.4,
10.0, 1.9, 0.1, 3.7, 0.4,
]
#convert this to a numpy array, and change the shape
data=np.array(datalist)
data=data.reshape(10,5) #changes the shape to get the data in the same format as table
#Get the colums out of the table:
x=data[:,0]#This is the first column of data (x)
t2=data[:,3]#The 4th column (t^2)
sigma_t2=data[:,4] #The 5th colum, sigma_t^2

#calculate the slope and offset
#start by calculating all of the sums
N=x.size
x2sum=(x*x).sum()
xt2sum=(x*t2).sum()
xsum=x.sum()
t2sum=t2.sum()

#We will use the mean value of the sigma_t2 as the "error in y"

denominator=N*x2sum-xsum**2
#slope:
m=(N*xt2sum-xsum*t2sum)/denominator
sigma_m=sigma_t2.mean()*sqrt(N/denominator)# using mean error in t^2
#offset:
b=(x2sum*t2sum-xsum*xt2sum)/denominator
sigma_b=sigma_t2.mean()*sqrt(x2sum/denominator)# using mean error in t^2

gmeas=2.0/m
sigma_g=2/m/m*sigma_m

#The fit results
text='''Fit results: t$^2$=mx+b
m= {:.3f} +/- {:.3f}
b= {:.3f} +/- {:.3f}
g= {:.1f} +/- {:.1f}'''.format(m,sigma_m,b,sigma_b,gmeas,sigma_g)

#calculate probability of obtaining a value this far away from the accepted value
dsigma=(9.81-gmeas)/sigma_g
prob=2.0*stats.norm.sf(dsigma)
print("Probability to get g {:.1f} sigma or further from accepted value: {:.3f}".format(dsigma,prob))

#plot:
#The data:
pl.errorbar(x,t2,yerr=sigma_t2,fmt='o',color='black')
#The fit:
pl.plot(x,m*x+b,color='red')
#The fit with m+/-sigma_c
pl.plot(x,(m+sigma_m)*x+b,'--',color='red')
pl.plot(x,(m-sigma_m)*x+b,'--',color='red')

pl.text(0,3,text,fontsize=12)#fit results onto the plot
pl.xlabel("x, drop distance (m)")
pl.ylabel("t$^2$, time squared (s$^2$)")
pl.title("Time to drop a certain distance")
pl.axis([-1,11,0,5])
pl.show()

\end{python}
\begin{poutput}
Probability to get g 2.2 sigma or further from accepted value: 0.027
(* \capfig{0.7\textwidth}{figures/gravmeasfit.png}{\label{fig:gravmeasfit} Measured time (squared) to fall a certain distance, using data from Table \ref{tab:gravmeas} and fit (solid red line) using equation \ref{eqn:LSmb}. The slope of the line is also shown for the values of the slope 1 standard deviation away from the mean value.} *)
\end{poutput}
The results of this code, are shown in Figure \ref{fig:gravmeasfit}. Since our formulas in equations \ref{eqn:LSmb} and \ref{eqn:LSmberror} assumed that all points had the same size vertical error bar, we used the mean of the values in the last column of table \ref{tab:gravmeas} as the error. We find that the measurement is $g=7.5\pm1.0\, m/s^2$ which is more than one standard deviation away from the accepted value of $g$ (9.81\,$m/s^2$). The value is $\frac{9.81-7.5}{1.0}\sigma=2.2\sigma$ away from the expected value, which has a 2.7\% change of occurring. It is not clear if this is due to pure chance or if there is a systematic effect. Given the low probability of obtaining such a shifted result, one should suspect a systematic effect. 

We should also note that the fit returned a non-zero value for the offset of the fit. Although our model expects the offset to be zero, we have found a value of $b=1.0\pm0.2\,s^2$ that is inconsistent with zero by 5$\sigma$. It is thus very likely that there is some systematic offset in the data leading to the time always being measured to be slightly high. Perhaps the person doing the timing systematically stopped the timer late, maybe due to their reaction time. This offset in the timing may also result in a systematic shift of the slope, leading to a systematic shift in the measured value of $g$. 

We see that even if we expect there to be no offset in the model, it is always best to include it. If there is no real systematic effect, then the offset should fit to a value that is consistent with zero. 

\end{example}

The analysis in example \ref{ex:gravLMfit} highlights some interesting aspects of ``fitting'' a straight line to data using the least squares method (also called ``linear regression''). In our physical model, \ref{eqn:t2ofx}, we would expect the data to fit to a straight line with a slope and no offset. It is generally true that even if the model does not have an offset, one should always fit a line with an offset to the data. If the data really are consistent with no offset, then the fitted value of the offset will be zero within its uncertainty. If the fitted offset is inconsistent with zero (as it was in the example by 5 standard deviations), then this is a useful tool to uncover a systematic effect. You should thus always include an offset in the model. 

\subsection{Residuals}
In order to get an idea of how well our model ``fits'' the data, one can look at the ``residuals'' of the fit. For a data point, $(x_i,y_i)$, with a model $y^{exp}(x)$, the residuals, $R(x_i)$, are defined as:
\begin{align}
R(x_i)\equiv y_i-y^{exp}(x_i)
\end{align}
and correspond to the difference between the data and the model prediction. Again, if we assume that the errors in the $x_i$ are negligible, then the error in the residuals are given by:
\begin{align}
\sigma_{R(x_i)}=\sigma_{y_i}
\end{align}
That is, the error on the residuals are given by the error in the data points. We do not propagate the error in the fit parameters to the residuals, as we are interested in the trend of the residuals and their distance away from zero in terms of the error in the data points. If the model is a good representation of the data, then the residuals are expected to be symmetrically distributed about a mean of 0. If all of the residuals were positive or negative, then one would question whether the model is a good fit to the data. Similarly, if the residuals show a trend, then one would also suspect that there is an issue with the model. The residuals for the fit from example \ref{ex:gravLMfit} are shown in Figure \ref{fig:gravmeasfit_res} and do not show any obvious trend. Within the uncertainties in the data points, we can conclude that a linear model is a good representation of the data. Note that in this particular experiment, we would expect that friction from drag would likely play an effect for high drop heights, and the residual plot may have helped us to identify this effect. The code to plot the residuals from example \ref{ex:gravLMfit} in python is given by:
\begin{python}[caption = Plotting residuals] 
#The residuals have the same error bars:
pl.errorbar(x,t2-(m*x+b),yerr=sigma_t2,fmt='o',color='black')
pl.xlabel("drop distance (m)")
pl.ylabel("residual from fit (s$^2$)")
pl.title("Residuals from linear fit")
pl.axis([-1,11,-1,1])
pl.show()
\end{python} 
\begin{poutput}
(* \capfig{0.7\textwidth}{figures/gravmeasfit_res.png}{\label{fig:gravmeasfit_res}Residuals from the fit in Figure \ref{fig:gravmeasfit}. There is no obvious trend in the residuals and we thus conclude the linear model is a reasonable representation of the data.} *)
\end{poutput}

\subsection{Correlated parameters}

In the least squares fitting for example \ref{ex:gravLMfit}, we treated the slope, $m$, and offset, $b$, as independent parameters. While the minimum in the chi-squared can be found for each parameter independently, it is not clear that the parameters are actually independent from each other. That is, it is likely the case that as $m$ changes, the corresponding value of $b$ that minimizes the chi-squared changes as well. In other words, it is usually the case that the parameters of the fit are correlated, which is important in considering their errors, especially when they are propagated to a quantity, $f(m,b)$, that depends on both $m$ and $b$. As you recall, we can only add errors in quadrature when propagating them if the various quantities with errors are independent of each other. If the value of $m$ that minimizes the chi-squared depends on $b$, then $m$ and $b$ are not independent and we would need to know their covariance to propagate their errors (equation \ref{eqn:coverror}).

This is illustrated for the example \ref{ex:gravLMfit} in Figure \ref{fig:gravmeasfit_chim} which shows the value of chi-squared as a function of $m$ for different values of $b$ (at the best fit value of $b$ and at the best fit value of $b$ plus 1 standard deviation). When $b$ is also at its best fit value ($b=1.0$), we see that the minimum of the chi-squared indeed occurs when $m$ is at its best fit value of $m=0.265$; however, this is no longer the case when $b=1.2$. We thus say the parameters $m$ and $b$ are correlated (rather, in this case, they are anti-correlated, as a larger value of one parameter leads to a lower optimal value of the other parameter). The $\chi^2$ is also shown in Figure \ref{fig:gravmeasfit_chi2d} as a function of both $m$ and $b$, and the tilt in the plot is a sign that the parameters are correlated.

The code to make the two plots in python is:
\begin{python}[caption = Plot chi-squared as a function of the fit parameters] 
#A function to calculate chi-squared for a given set of data, errors on data, and model predictions
def chis(ydata,sigma_y,ymodel):
    return (((ydata-ymodel)/sigma_y)**2).sum()

ydata=t2
sigma_y=sigma_t2

#an array of values of b and m centered around the best fit value:
b_vals=np.linspace(b-2*sigma_b,b+2*sigma_b,100)
m_vals=np.linspace(m-2*sigma_m,m+2*sigma_m,100)
    
#Plot the chi-squared vs m, for 2 different values of b    
pl.plot(m_vals,[chis(ydata,sigma_y,m_vals[i]*x+b) for i in range(m_vals.size)],label="b={:.1f}".format(b))
pl.plot(m_vals,[chis(ydata,sigma_y,m_vals[i]*x+(b+sigma_b)) for i in range(m_vals.size)],label="b={:.1f}".format(b+sigma_b))
pl.legend(loc='best')
pl.xlabel('m')
pl.ylabel('$\chi^2$')
pl.title('Minimum of $\chi^2$ as a function of m for different values of b')
pl.show()

#Make a 2D plot of chi-squared vs m and b:
mm,bb=np.meshgrid(m_vals,b_vals)
chi2d=pl.zeros(mm.shape)
for i in range(m_vals.size):
    for j in range(b_vals.size):
        chi2d[i,j]=chis(ydata,sigma_y,m_vals[i]*x+b_vals[j])
pl.pcolormesh(mm,bb,chi2d,cmap='RdBu')
pl.axis([m_vals.min(),m_vals.max(),b_vals.min(),b_vals.max()])
pl.xlabel('m')
pl.ylabel('b')
pl.title("$\chi^2$ as a function of m and b")
pl.colorbar()
pl.show()
\end{python} 
\begin{poutput}
(* \capfig{0.7\textwidth}{figures/gravmeasfit_chim.png}{\label{fig:gravmeasfit_chim}Chi-squared as a function of the slope from the fit in Figure \ref{fig:gravmeasfit} for different values of the offset. Since the minimum in chi-squared as a function of the slope, $m$, decreases for a higher value of the offset, $b$, we can see that the parameters from the fit are anti-correlated.} *)

(* \capfig{0.7\textwidth}{figures/gravmeasfit_chi2d.png}{\label{fig:gravmeasfit_chi2d}Chi-squared as a function of the slope and offset from the fit in Figure \ref{fig:gravmeasfit}. The ``tilt'' in the chi-squared map is the result of the anti-correlation between the two parameters.} *)
\end{poutput}

\section{Non-linear fits}
In the previous section, we used the principle of maximum likelihood and showed that minimizing the chi-squared, $\chi^2$, given by:
\begin{align}
\chi^2=\sum_{i=1}^{i=N}\frac{(y_i-y^{exp}(x_i))^2}{\sigma_{y_i}^2}
\end{align}
was equivalent to finding the maximum value of the likelihood. For the specific case of a linear  model, $y^{exp}(x)=mx+b$, we found that the slope and offset could be obtained analytically by differentiating the formula for the chi-squared. It can be shown that for any polynomial model, and indeed any model that is linear in the parameters that we want to determine, a closed form solution can be obtained from a system of linear equations. However, for model functions that are not linear in the parameters, one must determine the best fit parameter values numerically. In general, the problem is thus to find the values for a set of parameters, $\vec\beta=\{\beta_1, \beta_2,\dots\}$, such that the chi-squared:
\begin{align}
\chi^2=\sum_{i=1}^{i=N}\frac{(y_i-y^{exp}(x_i,\vec\beta))^2}{\sigma_{y_i}^2}
\end{align}
between the data and the model is a minimum. Various clever minimization algorithms exist to do this efficiently rather than systematically exploring all of parameter space. These algorithms are usually based on estimating the derivatives of the chi-squared as a function of the different parameters and searching in the direction with negative derivatives.

As an example, let us suppose that we have measured the energy of many collisions in an accelerator, and made a histogram of the energy of the collisions, between, say, 0\,MeV and 20\,MeV, in 1\,MeV bins (Figure \ref{fig:resdata}). This is a typical situation in particle physics, where we would want to  look for a resonance at a certain energy that would correspond to a specific particle being created. We expect the resonance to be a gaussian peak on top of a sloped background, and thus model the number of counts, $n$, as a function of energy, $E$, as:
\begin{align}
\label{eqn:resdataModel}
n(E)=\beta_0+\beta_1E+\beta_4\left( \frac{1}{\beta_3\sqrt{2\pi}}e^{-\frac{(E-\beta_2)^2}{2\beta_3^2}} \right)
\end{align}
where we have used $\beta_i$ for the 5 parameters, and labeled the first one with an index of 0 instead of 1, to be consistent with the code in python. Parameters 0 and 1 are thus the slope and offset of the background. Parameter 2 is the mean of the gaussian (the center of the resonance), parameter 3 is the width of the resonance (the standard deviation of the gaussian), and parameter 4 is the normalization of the gaussian (the number of events above background that are from the resonance). 

Since we can view each bin as a separate measurement, we can assign each bin an uncertainty equal to the square root of the number of counts in that bin (as you recall, the uncertainty in a counted quantity is the square root of the quantity). We thus wish to fit our model to the histogram of the number of counts as a function of energy to determine the slope and offset of the background, as well as the mean, standard deviation and normalization of the gaussian. An example of such data is shown in Table \ref{tab:resdata} and plotted in Figure \ref{fig:resdata}.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Energy, $E$}&\textbf{Counts in 1\,MeV, $n$}&\textbf{Uncertainty,$\sqrt{n}$}\\
\hline
1.0 & 4.0 & 2.0\\
\hline
2.0 & 3.0 & 1.7\\
\hline
3.0 & 2.0 & 1.4\\
\hline
4.0 & 3.0 & 1.7\\
\hline
5.0 & 5.0 & 2.2\\
\hline
6.0 & 2.0 & 1.4\\
\hline
7.0 & 4.0 & 2.0\\
\hline
8.0 & 2.0 & 1.4\\
\hline
9.0 & 10.0 & 3.2\\
\hline
10.0 & 16.0 & 4.0\\
\hline
11.0 & 12.0 & 3.5\\
\hline
12.0 & 6.0 & 2.4\\
\hline
13.0 & 3.0 & 1.7\\
\hline
14.0 & 3.0 & 1.7\\
\hline
15.0 & 3.0 & 1.7\\
\hline
16.0 & 3.0 & 1.7\\
\hline
17.0 & 5.0 & 2.2\\
\hline
18.0 & 5.0 & 2.2\\
\hline
19.0 & 2.0 & 1.4\\
\hline
20.0 & 5.0 & 2.2\\
\hline
\end{tabular}
\captionof{table}{\label{tab:resdata} Simulated data for a histogram of energy observed in an accelerator.}
\end{center}

\capfig{0.5\textwidth}{figures/resdata.png}{\label{fig:resdata}Simulated data from an accelerator experiment, showing the number of counts in 1\,MeV bins as a function of energy, from table \ref{tab:resdata}.}

We will use the \code{scipy.optimize} module to fit the model to the data and to determine the value of the unknown parameters. The first step is specify the model in python as a function of the dependent variable (in our case the energy) and the unknown parameters. We can make the definition of our function slightly more general by passing the unknown parameters as a tuple:
\begin{python}[caption = Defining a function for a non-linear fit] 
#Define a model for a gaussian + linear background, with 5 parameters:
def model(x, *pars):
    offset=pars[0]
    slope=pars[1]
    mu=pars[2] #mean
    sigma=pars[3] #sigma
    norm=pars[4] #normalization
    return offset+slope*x+norm*(1.0/sqrt(2.*pi*sigma**2)*np.exp(-((x-mu)**2)/2./sigma**2))
\end{python}
We can load the data into arrays, \code{xdata}, \code{ydata}, and \code{ysigma}, corresponding to the 3 columns in Table \ref{tab:resdata}. The \code{curve\_fit} function can then be used to fit the data for the unknown parameters:
\begin{python}[caption = Non-linear fit in python] 
from scipy.optimize import curve_fit 
#Try a guess for our parameters
#The fit does not converge if we give a crazy guess for the mean
guess_pars=[1,1,12,1,1]

#Run the fit to the data:
fit_pars, fit_cov = curve_fit(model,xdata,ydata,sigma=ysigma,p0=guess_pars)
\end{python}
where we have to ``help'' the algorithm a little by specifying reasonable guesses for our unknown parameters (in this case, the algorithm can have trouble if the guess for the mean of the gaussian is unreasonable). Note that we can specify a different error bar for each data point (which we did by passing the y errors on each point using the array \code{ysigma}) . 

The \code{curve\_fit} function returns a tuple with two components. The first component, \code{fit\_pars}, is an array with the best fit values of the parameters (e.g. \code{fit\_pars[0]} corresponds to the offset of the sloped background). \code{fit\_cov} is the ``covariance matrix'' of the fit parameters. Each diagonal element in the covariance matrix corresponds to the variance of that parameter. Thus the square roots of the diagonal elements correspond to the standard deviations of each parameter estimate and can be used as the uncertainty in the corresponding parameter. The off-diagonal elements correspond to the covariance factors between pairs of parameters (for example, the element in the second row and third column of the matrix corresponds to the covariance between the second and third parameter in the fit). The covariance matrix is thus symmetric. If two parameters have a large covariance, then the determination of one of those parameters has a strong influence on the other. 

The following python code performs a fit to the data in Table \ref{tab:resdata} using our model, produces a plot showing the data with the fit and the residuals (Figure \ref{fig:resdata_fit}), and calculates the chi-squared for the fit. 
\begin{python}[caption = Complete non-linear fit with residuals] 
import numpy as np
from math import *
import pylab as pl
from math import *
from scipy.optimize import curve_fit 
import matplotlib.gridspec as gridspec  # for unequal plot boxes

#Define a model for a gaussian + linear background, with 5 parameters:
def model(x, *pars):
    offset=pars[0]
    slope=pars[1]
    mu=pars[2] #mean
    sigma=pars[3] #sigma
    norm=pars[4] #normalization
    return offset+slope*x+norm*(1.0/sqrt(2.*pi*sigma**2)*np.exp(-((x-mu)**2)/2./sigma**2))
    
#Load the table data
data=np.array([1.0,4.0,2.0,
2.0,3.0,1.7,
3.0,2.0,1.4,
4.0,3.0,1.7,
5.0,5.0,2.2,
6.0,2.0,1.4,
7.0,4.0,2.0,
8.0,2.0,1.4,
9.0,10.0,3.2,
10.0,16.0,4.0,
11.0,12.0,3.5,
12.0,6.0,2.4,
13.0,3.0,1.7,
14.0,3.0,1.7,
15.0,3.0,1.7,
16.0,3.0,1.7,
17.0,5.0,2.2,
18.0,5.0,2.2,
19.0,2.0,1.4,
20.0,5.0,2.2])
#extract the data by column
data=data.reshape(20,3)
xdata=data[:,0]
ydata=data[:,1]
ysigma=data[:,2]

#Try a guess for our parameters
#The fit does not converge if we give a crazy guess for the mean
guess_pars=[1,1,12,1,1]

#Run the fit to the data:
fit_pars, fit_cov = curve_fit(model,xdata,ydata,sigma=ysigma,p0=guess_pars)

#Get the parameter errors out of the covariance matrix:
fit_err = np.sqrt(np.diag(fit_cov))

#Calculate value of y from the model using the fit parameters:    
yfit=model(xdata,*fit_pars)   

#Calculate the residuals:
yres=ydata-model(xdata,*fit_pars)

#Calculate the chi-squared between data and the fit
chisq=(((ydata-yfit)/ysigma)**2).sum()
ndof=ydata.size-fit_pars.size-1

#These are the "true" parameters used to make the data (for comparison)
true_pars=[3.,0.1,10.,0.8,20.]

#Put the results into some text:
textfit="Fit (true) parameters, $\chi^2$={:.1f}, ndof={}:\n".format(chisq,ndof)
textfit=textfit+"n(E)=p[0]+p[1]+p[4]*Gauss(E,p[2],p[3])\n"
for i in range(len(true_pars)):
    textfit = textfit + "Par {}: {:.2f} +/- {:.2f} ({:.2f})\n".format(i,fit_pars[i],fit_err[i],true_pars[i])

#Plot the fit and the residuals
pl.figure(figsize=(8,8))
gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])
pl.subplot(gs[0])
pl.plot(xi,model(xi,*fit_pars) ,lw=2,color='red')
pl.errorbar(xdata,ydata,yerr=ysigma,fmt='o',color='black')#data
pl.ylabel('n, number of collisions in 1 MeV bin')
pl.title('Fit to the fake data')
pl.text(-0.5,13,textfit,fontsize=12)
pl.axis([-1,22,-2,25])

pl.subplot(gs[1])
pl.errorbar(xdata,yres,yerr=ysigma,fmt='o',color='black')#residuals
pl.ylabel('residuals')
pl.xlabel('E, energy of the collisions (MeV)')
pl.axis([-1,22,-6,6])

pl.show()
\end{python}
\begin{poutput}
(* \capfig{0.7\textwidth}{figures/resdata_fit.png}{\label{fig:resdata_fit} Fit to the data from Table \ref{tab:resdata} and residuals using the model in equation \ref{eqn:resdataModel}. The ``true'' value of the parameters used to make the data are shown in parentheses next to their fitted values.} *)
\end{poutput}

By looking at the residuals of the fit, we see no evidence of a trend in the residuals that would indicate that this model is not a good fit for the data. We can conclude from the fit results that the data are consistent with a resonance near 10\,MeV with a confidence level of 6$\sigma$, as evidenced by the uncertainty in the normalization of the gaussian (the fit indicates that the normalization of the gaussian is inconsistent with zero by 6 standard deviations).

\section{Goodness of fit}
So far, we have concerned ourselves with determining the parameters of a model that best fit a set of data.  We have qualitatively looked at the residuals between the fit and the data to look for a trend in the residuals that would indicate that the model is inadequate. However, we have not determined quantitatively if the model is a \textit{good} fit to the data. Since this is a vast subject, we will only limit ourselves to one method to evaluate the quality of a fit: the value of the $\chi^2$. 

We know that we can determine the parameters of the model, $\vec\beta$, by minimizing the chi-squared between the data and the model. It turns out that we can use the value of the chi-squared evaluated using the best-fit parameters, to tell us about the goodness of the fit. Recall the definition of the chi-squared:
\begin{align}
\chi^2=\sum_{i=1}^{i=N}\frac{(y_i-y^{exp}(x_i,\vec\beta))^2}{\sigma_{y_i}^2}
\end{align}
As we discussed previously, we expect that for a good model, the chi-squared will increase by approximately 1 per data point. On average, we expect the data points to differ from the model prediction by approximately 1 standard deviation; thus each term in the sum contributes approximately 1 to the chi-squared.

We now introduce the ``number of degrees of freedom'', $\nu$, for the fit as:
\begin{align}
\nu\equiv N-n-1
\end{align}
where $N$ is the number of data points and $n$ is the number of parameters that we are fitting, and the minus 1 is related to the $N-1$ that we have in the definition of the variance. The number of degrees of freedom tells us something about how much we can vary the parameters in the model based on the number of data points. For example, if we have 2 data points we can always find a straight line (with two parameters, slope and offset) that will exactly go through those two points, and we really have no freedom to vary those parameters. In this situation, we have no ``degrees of freedom'' (in fact we have $\nu=-1$ because of the ``minus 1'' in the definition of $\nu$). In a case with no (or few) degrees of freedom, we cannot really say much about how our model fits the data, since there is very little liberty in our model. Conversely, if our model appears to fit the data well when we have a large number of degrees of freedom, we can be more confident in our model. 

Although it is beyond the scope of this text, one can show that the chi-squared should scale roughly as the number of degrees of freedom, rather than the actual number of data points. We can define the ``reduced chi-squared'', as the chi-squared divided by the number of degrees of freedom. In the fit from Figure \ref{fig:resdata_fit}, we found a chi-squared of 6.8 for 14 degrees of freedom. The reduced chi-squared was thus 6.8/14=0.5. If the reduced chi-squared was substantially bigger than 1, we would conclude that the model is not likely a good fit to the data. One should however be careful: a small reduced chi-squared does not necessarily imply that the model is a good fit! If you think about it, the denominator in the chi-squared is the error on each data point. Thus, if the errors are very large, the chi-squared will be small, and we cannot conclude anything about the model since the errors in the data are too large.

We can be quantitative about the expected value of chi-squared for a given number of degrees of freedom. Let us suppose that our 5 parameter model from equation \ref{eqn:resdataModel} is in fact the correct model for the data from our experiment. That is, if we repeat the experiment a number of times, we will be able to fit the data to our model and obtain a ``reasonable'' chi-squared. However, each time that we repeat the experiment, due to statistical variations in the data, we will not obtain exactly the same chi-squared; rather, we will obtain a distribution of different values of chi-squared. The distribution of chi-squared that we obtain is well-understood and governed by the ``chi-squared distribution'', given by the following probability density function:
\begin{align}
P^{\chi^2}(x,\nu)=\frac{x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{\nu}{2}}\Gamma\left(\frac{\nu}{2}\right)}
\end{align}
where $\Gamma()$ is called the ``gamma function''. Figure \ref{fig:chi2n} shows the chi-squared distribution for different number of degrees of freedom.
\capfig{0.7\textwidth}{figures/chi2n.png}{\label{fig:chi2n} Probability density functions for the chi-squared distribution for different numbers of degrees of freedom.}

The chi-squared distribution for 14 degrees of freedom is shown in Figure \ref{fig:chi2n14}. For our example from Figure \ref{fig:resdata_fit}, we obtained a chi-squared of 6.8. By looking at the cumulative chi-square distribution in Figure \ref{fig:chi2n14}, we can see that the probability of obtaining a $\chi^2=6.8$ \textit{or less} is 6\%. Conversely, there is a 94\% chance of obtaining a chi-squared of 6.8 or bigger simply from statistical variations in the data if the model is correct. This effectively informs us about the quality of our fit. If the model is correct, we got lucky and obtained a very good fit, and 94\% of the time, we would get a worse chi-squared. We can define the ``p-value'' to be 1 minus the cdf for the chi-squared distribution (which we also called the ``survival fraction'' in general), which in this case is 94\%.

The p-value is the probability of obtaining a chi-squared equal or worse (bigger) than we did, if the model were correct. We have to make our decision on whether we have a good fit based on a statistical criterion. Typically, we could say that if the p-value is less than 5\%, then the model is not likely to be a good fit. But again, we should take this with a grain of salt and generally make sure that we understand everything about the fit (such as the residuals and the covariance matrix) rather than making a conclusion based on a ``hard'' threshold on the p-value.

\capfig{0.7\textwidth}{figures/chi2n14.png}{\label{fig:chi2n14} Probability density and cumulative density functions for the chi-squared distribution for 14 degrees of freedom (as in the case for the fit in Figure \ref{fig:resdata_fit}).}

\section{Summary}
In this chapter, we used the principle of maximum likelihood to evaluate the parameters, $\vec\beta$, of a model, $y^{exp}(x,\vec\beta)$, to fit a set of $N$ pairs of data points, \{$(x_1,y_1), (x_2,y_2), \dots, (x_N,y_N)$\}. We found that this led to the requirement that we minimize the chi-squared, $\chi^2$, given by:
\begin{align}
\chi^2=\sum_{i=1}^{i=N}\frac{(y_i-y^{exp}(x_i,\vec\beta))^2}{\sigma_{y_i}^2}
\end{align}
which is a sum of the squared distance between the points and the model, divided by the error in the data points. For this reason, the method is also called the ``Method of Least Squares''. We note that this \textbf{method is only valid when we expect the errors on the $y$ points to be standard errors} (i.e. that the uncertainties on the $y$ values correspond to the standard deviation of values that are normally distributed about the model prediction, $y^{exp}$).

In the case where the model is linear in the unknown parameters, one can minimize the chi-squared analytically (by taking the derivatives with respect to the parameters and setting them equal to zero) and obtain a set of coupled linear equations for the parameters. We showed this explicitly for the model:
\begin{align}
y^{exp}(x,m,b)=mx+b
\end{align}
where we found that the least squares estimates of $m$ and $b$, if all of the errors in the $y_i$ are equal to $\sigma_y$, are given by:
\begin{align}
m=\frac{N\sum xy-\sum x\sum y}{N\sum x^2-\left(\sum x\right)^2}\nonumber\\
b=\frac{\sum x^2\sum y-\sum x \sum xy}{N\sum x^2-\left(\sum x\right)^2}
\end{align}
Using the formalism to propagate errors, we found that the errors in the parameters are given by:
\begin{align}
\sigma_m&=\sigma_y\sqrt{\frac{N}{N\sum x^2-\left(\sum x\right)^2}}\nonumber\\
\sigma_b&=\sigma_y\sqrt{\frac{\sum x^2}{N\sum x^2-\left(\sum x\right)^2}}
\end{align}
We found that we could qualitatively discuss the goodness of a model by considering the residuals between the model estimates of the $y_i$ and the measured values. If the residuals show no trend and are symmetrically distributed about 0, then the model is likely a good representation of the data.

We also argued that there may be correlations between fitted parameters, and that one must take this into account (through their measured covariance), if propagating the errors from the parameters.

We also considered the general case of models that are not linear in the parameters, and showed that one can use numerical techniques to minimize the chi-squared in theses cases, and to calculate the covariance matrix.

We finished by discussing one method to evaluate the goodness of the fit of the model to the data. We saw that the value of the chi-squared itself is a good indication of the fit. In particular, one expects that the chi-squared should be equal to approximately the number of the degrees of freedom in the model, $\nu$:
\begin{align}
\nu\equiv N-n-1
\end{align}
where $N$ is the number of data points and $n$ is the number of parameters. We saw that if a model is correct, we will obtain a well-defined distribution of chi-squared values when the model is fit to various data sets with statistical fluctuations. We introduced the p-value as the probability that if the model is correct, one would obtain an equal or worse value of chi-squared (the p-value is one minus the cdf of the chi-squared distribution). We cautiously recommended that in general, if the p-value is less than 5\%, then the model is unlikely to be a good representation of the data.
