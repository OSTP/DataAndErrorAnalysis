%Copyright 2016 R.D. Martin
%This book is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
%
%This book is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details, http://www.gnu.org/licenses/.
\chapter{Determining uncertainties and presenting data}
In this chapter we will look at how to determine uncertainties on measurements, how to present those uncertainties, what they mean, and how to determine if two values agree. We will give some basic guidelines that are of use for undergraduate laboratories, and justify most of those guidelines in later chapters.

\section{The importance of uncertainties}
The importance of uncertainties is fundamental in applying the Scientific Method, where one tries to invalidate a hypothesis by using data. Generally, we can think of the need to compare two numbers that cannot be determined to infinite precision (e.g. a measurement and a theoretical prediction). We thus need a way to scientifically determine whether two quantities are ``consistent'' with each other, when they are not exactly equal. We also need to understand how to present a number; if our ``value determined from experiment'' involves an operation that leads to many digits (e.g. multiplying by an irrational number), we will have to decide how many digits are really ``significant'' before performing a comparison.

You may find that you can measure a number to several decimals with an accurate ruler, but then are unable to reproduce that measurement to the same accuracy. Even if the ruler can measure something to a certain number of decimals, the uncertainty in the measurement may not be representative of that precision. For example, consider using a micrometer (which can measure distances of order 10\,cm to sub-millimetre accuracy) to measure your height; you will inevitably introduce errors when you try to stack the micrometers atop each other to reach your height. Inituitively, the uncertainty in your height should be larger than the precision of the micrometer, even though that is the only instrument that was used.
 
The need to assign uncertainties to numbers also arises when we try to build things. If you are cutting pieces of wood to build furniture, you will need them to fit together. You will find that you cannot cut them to be exactly the length that you determined in your drawings (even when you take into account the width of the saw blade, use a good square, work very carefully, etc.). You will find that you need to determine the ``tolerance'' in all of your required dimensions for the parts to fit. If you go to a professional machine shop to have parts manufactured, the machinists will inevitably ask you what tolerance you have on the pieces; it is usually not their job to assemble your parts for you. This issue becomes one that is quite important in designing parts; it may be relatively easy to design the function of a part compared to determining how much tolerance is required for the parts to fit. Although the issue of tolerance is related to uncertainties, in this book, we will focus more on the aspect of comparing two numbers, but do keep in mind that the concepts usually apply to tolerances when designing parts.

\subsection{What do we mean by ``uncertainty''?}
You have likely already encountered uncertainties in your labs, but you may not really have thought them through. A typical experiment could be to build a simple balance, where two unequal weights are placed on either side of a lever arm. We may want to determine where to place the weights to equilibrate the balance. We will need to measure the masses of the weights, and if the digital scale displays numbers down to a gram, we would likely quote the weight of one of the masses as, say,  $m_1$=(595.0 $\pm$ 0.5)\,g. We should ask ourselves what we really mean by (595.0 $\pm$ 0.5)\,g? Do we mean that the mass of the weight is \textit{guaranteed} to be between 594.5\,g and 595.5\,g? Does it mean we would bet our lives on it? What if we later discovered that the scale reads all weights to be 5\,g too light, can we guard ourselves against that (since we are apparently willing to bet our lives on our measurements)?

The basic answer to these questions is that we cannot just quote the measured value to be $m_1$=(595.0 $\pm$ 0.5)\,g! It is \textit{on us}, the scientists, to explain what we mean by $m_1$=(595.0 $\pm$ 0.5)\,g. If we mean that we just assigned an uncertainty, without much thought, of 0.5\,g because that is half of the smallest digit on the scale, then we have to say that. If we mean that we would bet our lives that the true value is between 594.5\,g and 595.5\,g, then we should say that. If we mean that we are 90\% confident that the true mass lies between 594.5\,g and 595.5\,g, then we should say that (but then, what do we mean by 90\% confident?). If we are not confident that the scale reads ``true'' (e.g. because we did not repeat our measurement with a different scale, or do not know if the scale was calibrated), then we should mention that as well. Again, we see that doing good science is about being precise! Furthermore, doing good experimental physics, is all about \textit{understanding} uncertainties.

You may have also seen results from opinion polls that try to survey the general population to predict election results. These polls can be quite important in how politicians plan their campaigns (read: where to spend money), and the polls may be quite expensive to conduct, so they typically are motivated to get scientific data. You may, for example, see the following reported in a news article: ``Candidate A is leading, favoured by 52\% of voters polled, whereas Candidate B has fallen behind and is favoured by only 48\% of voters''. At the end of the article, there will typically be a statement of the form: ``The poll was conducted by BMIP by calling 1000 registered voters and conducting a 10 question survey by phone. The margin of error on the polling numbers are 4\%''.

We can ask ourselves what they mean by the ``margin of error''. It certainly makes sense that they cannot get an exact number for the whole electorate by surveying only 1000 people, as there will necessarily be statistical fluctuations in the sample that they surveyed. We can ask ourselves if the difference between the two candidates really is ``significant''. If the results really are (52 $\pm$ 4)\% and (48 $\pm$ 4)\%, there is substantial overlap between the possible results and a statistical fluctuation cannot be excluded. Do they mean that for Candidate A, the result is definitely between 48\% and 56\% with an equal probability over that whole range? How did they determine the 4\% uncertainty? Why did they ask 10 questions when they only needed one?

Although we will not spend any time on designing opinion polls, we will spend a substantial amount of time trying to understand the uncertainties in statistical quantities. As you can see, a strong understanding of statistical uncertainties is not only useful in physics, but carries out to a wide range of applications in real life, where we are often confronted with dubious conclusions from badly analysed data.

All this being said, when we quote a number as $A\pm \sigma_A$, we imply that $A$ is quite likely to be in the range $A-\sigma_A$ to $A+\sigma_A$ (and we typically specify what we mean by ``quite likely''). Generally, we also imply that it is \textbf{not} more likely for $A$ to be closer to one of the end points of the range than the other. We always quote the uncertainty, $\sigma_A$, as a positive number. We call the quoted value of $A$ the ``best estimate'' or ``central value'', and $\sigma_A$ as the ``uncertainty'' or ``error''.

\subsection{Systematic uncertainties}

Systematic uncertainties are different from what we would call ``statistical'' or ``random'' uncertainties. By \textbf{random uncertainty}, we mean that a repeated measurement will give variable results that are \textbf{equally likely to be higher or lower than the true value}. With a \textbf{systematic error, we mean that reapeated measurements will be systematically higher or lower measurements than the true value} (hence the name). The advantage of a random error is that if we make many measurements, we can expect that the average of those measurements will tend to the true value. There are no advantages to systematic errors! They are often difficult to detect and the design of most experiments is often driven by the need to reduce systematic uncertainties, ideally to the level that they are negligible.

For example, imagine that several laboratory groups are measuring weights for an experiment. They all have their own scales, and as good experimentalists, they borrow each other's scales to check their measurements and make sure that their own scale is calibrated. However, all of the scales were calibrated by the TA before the lab, and the TA used the wrong ``standard weight'' when calibrating all of the scales. This is an example of a systematic uncertainty in all of the mass measurements in the lab that is very difficult to catch. Or maybe the scales are more accurate for weights that are smaller than 100\,g, and the various lab groups have used weight well below and well above 100\,g; some groups may be affected by the systematic uncertainty, while others not. 

Generally, it is not possible to arrive at a prescription of how to determine the systematic uncertainty in a measurement. It is however very important to think very carefully about all possible sources of systematic uncertainties. Ideally, this is done before designing an experiment so that the design of the experiment minimizes the associated systematics. One can then typically estimate the magnitude of the systematic uncertainties and conduct the experiment with the knowledge that the most precise result that they can obtain will have an uncertainty at least as big as the systematic uncertainty.

Experiments that are currently searching for dark matter are a good example of how the minimization of systematic uncertainties are critical in the design of the experiment. These experiments require building large sensitive detectors to look for small flashes of light resulting from dark matter particles interacting in the detector. The problem is that particles emitted from naturally occurring radioactive decays in the materials used to construct the experiments will also produce flashes of light. Therefore, those experiments can only be sensitive to dark matter if the rate of the flashes of light from dark matter is substantially higher than the rate from ``backgrounds''. Much of the effort in designing these experiments is thus spent in minimizing the backgrounds (e.g. by material selection) so that the sensitivity to dark matter is as high as possible, and not ``systematically'' limited by a high background rate. The experiments are then conducted until the sensitivity to dark matter becomes limited by the background rate.

\section{Prescription for determining certain uncertainties}
In certain situations, one can use fairly well prescribed methods to specify the random component of the uncertainty in a given measurement. We cover those particular cases in this section.

\subsection{Uncertainties from scales - ``Half of the smallest division''}
We first consider the uncertainty in measurements made with a device that has some sort of scale on it (a ruler, a digital scale, a needle on a dial, etc.). We assume that the measuring device is well calibrated, but in a real application, we should estimate whether we also need to include a systematic uncertainty from the calibration. 

If we have a ruler that has \,mm graduations on it, we should expect that we can measure a distance to within 1\,mm. In a typical scenario, we will use the ruler and find that the true value is between two lines on the ruler (Figure \ref{fig:ruler}). In this case, we would typically quote the best estimate of the measurement to correspond to the line on the ruler \textbf{that is closest to the distance being measured, and an uncertainty that is half of the smallest division on the ruler}. Figure \ref{fig:ruler} shows an example of measuring something with a ruler. Using our prescription, we would quote the grey object to have length of (2.80 $\pm$ 0.05)\,cm. That is, we are reasonably confident that the true length is between 2.75\,cm and 2.85\,cm, which is a range of 1\,mm, corresponding to the precision of the ruler. 

\capfig{0.4\textwidth}{figures/ruler.png}{\label{fig:ruler} Measuring with a ruler.}

This general rule (``half of the smallest division'') can be used for all measurements devices where numbers are read off a scale, including digital scales. For example, a digital voltmeter may read 12.1\,V (0.1\,V being the smallest displayed digit) and we would then quote the measurement as (12.10 $\pm$ 0.05)\,V. Sometimes, it is clear that we can provide a measurement with a higher accuracy than half of the smallest division (if the divisions are far apart), but we should only do so cautiously. Even using half of the smallest division may significantly underestimate the uncertainty (for example, we may not be able to get the ruler right up against what we are trying to measure, or what we are trying to measure has a finite width that makes it unclear from which point to measure, or the needle on the dial is fluctuating in position, or the digits on the voltmeter are fluctuating). Often, we can get the best idea of whether our uncertainty is reasonable by repeating the measurement, and we should always do so if we can. 

With this type of uncertainty, we are usually almost 100\% confident that the true value lies in the quoted range. However, since it is very difficult to guarantee that there was no systematic effect (e.g. parallax, non-calibrated instrument, clearly defined what we need to measure), it is not a good idea to treat even these types of measurements as giving us absolute confidence in the quoted uncertainty range. It is generally safer to treat even this type of uncertainty with a confidence around the typical 68\%, unless repeated measurements and control of systematic effects give us reason to be more confident.

\subsection{Repeatable measurements - ``Mean and uncertainty on the mean''}
When we have the opportunity to repeat measurements, we should always do so. If we have several measurements of a single quantity, then how do we define the central value and the uncertainty? For example, we may be using a stop watch to measure the period of a pendulum, and obtained several values, say $T$=\{2.1\,s, 2.2\,s, 2.1\,s, 2.0\,s, 2.3\,s, 2.1\,s\}, so what should we quote as our measured value? It is likely that the true value is somewhere between 2.0\,s and 2.3\,s, so we could quote the result as (2.15 $\pm$ 0.15)\,s (given by the central value of the range with an uncertainty that spans the whole range). In later chapters, we will motivate a different prescription, but we take it here as given:

\textbf{When we have a set of $N$ independent measurements, \{$x_1, x_2, x_3, \dots, x_n$\} of a quantity, $x$, the quoted result should be given as $\bar x \pm \frac{\sigma_{\bar x}}{\sqrt{N}}$}, where
\begin{align}
\label{eqn:MeanAndStd}
\bar x &\equiv \frac{1}{N}\sum x_i \nonumber\\
\sigma_x &\equiv \sqrt{\frac{1}{N-1}\sum (x_i-\bar x)^2}\nonumber\\
\sigma_{\bar x} &\equiv \frac{\sigma_x}{\sqrt{N}}
\end{align}
$\bar x$ is the algebraic average (or ``mean'') of the values, and $\sigma_x$ is called the ``standard deviation'' of the measurements (and requires the mean to be known). The standard deviation is a measure of the average distance between the individual measurements and the mean. It is also representative of the uncertainty on a single measurement. A large standard deviation means that the measurements are spread out over a larger range about the mean.

$\sigma_{\bar x}$ is called the ``error on the mean'', and corresponds to the uncertainty in determining the mean of the measurements, which is smaller than the standard deviation (since it is the standard deviation divided by $\sqrt{N}$). It makes sense for the error on the mean to be smaller than that of the individual measurements because it combines the information from multiple measurements. Again, here we assumed that there is no systematic uncertainty that skewed all of the measurements and that the measurements are independent of each other.

The uncertainty that we obtain using the error on the mean does not guarantee that the true value is within the quoted range. In fact, it is carefully designed (under most circumstances) so that there is a 68\% chance that the true value lies within the specified range. We will explain why we use 68\% in chapter \ref{chap:StatsNormal}.

\begin{example}{0pt}{What is the central value and uncertainty on $T$, if the following measurements were performed: $T$=\{2.1\,s, 2.2\,s, 2.1\,s, 2.0\,s, 2.3\,s, 2.1\,s\}?}{}
\label{ex:ChapUncertainties_mean}
This is done easily in python
\begin{lstlisting}[frame=single] 
import numpy as np
from math import *

#load the measurements into a numpy array
Ti = np.array([2.1,2.2,2.1,2.0,2.3,2.1])

#numpy already knows how to find the mean and standard deviation:
Tavg = Ti.mean()
Tstd = Ti.std(ddof=1)# this results in the N-1 in the denominator instead of N
Terr = Tstd/sqrt(Ti.size)
print("Mean = {:.2f}, Standard deviation = {:.2f}, Error on the mean = {:.2f}".format(Tavg, Tstd, Terr))
print("T = ","{:.2f} +/- {:.2f} s".format(Tavg,Terr))
\end{lstlisting}
The output is:
\begin{verbatim}
Mean = 2.13, Standard deviation = 0.10, Error on the mean = 0.04
T =  2.13 +/- 0.04 s
\end{verbatim}
Or, using QExpy:
\begin{lstlisting}[frame=single] 
import qexpy as q
T = q.Measurement([2.1,2.2,2.1,2.0,2.3,2.1])
q.set_sigfigs(2)
print("T = ","{:.2f} +/- {:.2f} s".format(T.mean,T.error_on_mean))
\end{lstlisting}
The output is:
\begin{verbatim}
T =  2.13 +/- 0.04 s
\end{verbatim}

\end{example}


\subsection{Counting experiments - The square root rule}
\label{sec:countingError}
In some experiments, the quantity that we measure is something that we can count. For example, we may want to figure out the average rate of car accidents per month in our town. We may, for example, count that in January, there were 8 accidents. That is a definite number without an obvious uncertainty. However, we would likely be wrong if we claimed that every month, there will be exactly 8 accidents. So how do we translate our measurement of 8 counts into a more scientific statement, such as ``\textit{In our town, we determined the average rate of accidents per month to be 8 $\pm$ X}''? 

It turns out that the answer is rather simple: \textbf{if the events occur at random times but with a well defined average rate, the uncertainty on the number of counts, $N$, is the square root of the number of counts, $\sqrt{N}$.}. In our example, we would conclude that the rate of accidents per month is 8 $\pm$ 2.8. The square root uncertainty is also constructed such that the true value has approximately a 68\% chance of occurring in the quoted range. The value is close to 68\% when $N$ is large (bigger than about 75), and reduces as $N$ becomes smaller. We will examine this in more detail in Chapter \ref{chap:StatsDistributions}, when we consider the Poisson distribution.

\section{Overestimating uncertainties is also bad!}
With all of these caveats in determining the uncertainties on measurements, we may be tempted to just round up all of our uncertainties to be on the safe side. That way, we will have a good chance of concluding in our lab report that the results agree with the laws of physics! In principle, for an undergraduate laboratory, there will not be much consequences in doing this (the labs are not really expected to revolutionize our understanding of physics). Hopefully, there is no need to convince you that this is not a very scientific attitude, and you certainly should not think like this! Similarly, it should be obvious that if we really are testing the laws of physics (with the hope of discovering something new), then we really want to have the most precise result possible, with the smallest, but justifiable, uncertainties.

If we understand our uncertainties and have defined them precisely, then we would only be tempted to inflate them after the fact, when we see that our results disagrees with the hypothesis that we are testing (or do not lead to the expected conclusion). If our uncertainties are well defined and our result does not agree with the hypothesis, then we must have missed a systematic effect. In fact, we can tell from the disagreement between our result and the hypothesis how big that systematic effect must be. This is in fact an opportunity to think through the experiment and try to identify where the systematic error arose (or find the mistake in our calculations).

This approach of ``discovering a systematic'' uncertainty can apply in undergraduate laboratories, where we are highly confident that we will not find an inconsistency with the laws of physics that we are testing (and thus that we can reasonably expect a certain result). However, in ``real'' experimental physics, one does not have a hypothesis that can be taken as ``definitely correct'' (or rather, ``tested to a much higher degree of accuracy by someone else'') and this is why systematic uncertainties are particularly dangerous, since there is no guaranteed way to find them. This is also why, in general, we want to make sure that our uncertainties are not overestimated, so that others can verify our results as accurately as possible. Please become a good scientist and do not overestimate your uncertainties!


\section{Reporting measurements, significant figures}
In the above example (Example \ref{ex:ChapUncertainties_mean}), we asked python to format the output to 2 decimal places. By default, python would have output the result as $T$ = (2.13333333333 $\pm$ 0.0461880215352)\,s. In this case, it would not make sense to present that many decimal places on the central value, when the uncertainty is much bigger than most of them. Similarly, it does not make sense to claim that we have determined the uncertainty that precisely either. We should quote results and uncertainties showing only ``significant'' digits, and in a way that is the most legible (for example by using scientific notation).

Again, we do not have a perfect rule for determining the significance of digits, but in general, \textbf{the uncertainty should have 1 (and rarely more) ``significant figures''}. By significant figure, we mean digits after leading zeros or before trailing zeros. For example, in the number 00019.21000, only 19.21 are ``significant'', and we say that the measurement is determined to four significant figures. Once the number of significant figures is determined on the uncertainty, one can quote the central value with the same number of figures. When we quote a number, we should ask ourselves if we can justify all of the decimals, or if they are instead arbitrary (maybe from a calculation or from a random fluctuation in the apparatus); if we do not think that we can reproduce a measurement to all of the significant figures that we quoted, then we should consider them as non-significant and not present all of them.

Usually, we are interested in evaluating the precision of a result; that is, how big the uncertainty is relative to the central value. The ``relative'' uncertainty (also called the ``fractional uncertainty'') is usually a good measure of the precision, and should generally be quoted with a measurement. The \textbf{relative uncertainty is quoted as a percent and is given by the uncertainty divided by the central value}.

In order to help our audience appreciate the precision of our measurement even without giving the relative uncertainty, we should present the number using scientific notation and factoring out the units in a way that it is easy to estimate the relative uncertainty. For example, a measurement of $m$=12000\,g $\pm$ 23\,g, could be presented as $m$=(120.00 $\pm$ 0.23) $\times 10^2$\,g; that way, it is quite obvious that the relative uncertainty is of order 0.2\%.

Generally, if the first significant digit in the uncertainty is small (say, less than 5) and the second digit is large (say, more than 5), then it may make sense to include a second digit since that digit results in an ``appreciable change'' in the uncertainty. For example if an uncertainty is determined to be 0.15, then it might make sense to quote it as 0.15 instead of 0.2, since the additional decimal changes the uncertainty by 25\% (an arguably appreciable change). Conversely, if the uncertainty is determined to be 0.92, then it might as well be quoted as 0.9, since the 0.02 results in a small relative change in the uncertainty. In addition to considering the relative change in the uncertainty itself when deciding whether to include an additional digit, we can consider the change in the relative uncertainty of the measurement. For example, if a measurement is determined to be 10001.12 $\pm$ 0.15 (a relative uncertainty of $\frac{0.15}{10001.12}=0.001\%$), then quoting it as 10001.0 $\pm$ 0.2 probably does not make a significant difference in our conclusion. So what is an ``appreciable relative change''? That is of course arbitrary, but for high precision measurements anything above a 1\% change may be considered appreciable, while in undergraduate laboratories, that threshold could be closer to 5\%. 

\begin{example}{0pt}{Present the following results using appropriate significant figures and give the relative uncertainties on the results: 2.1124 $\pm$ 0.156, 100000 $\pm$ 3900, 0.000084341387 $\pm$ 0.000000322331, 0.000091 $\pm$ 0.1}{}
\begin{itemize}
 \item $2.1124 \pm 0.156 \rightarrow 2.11 \pm 0.16$ (8\% relative) - Include a second digit in the uncertainty since it is an appreciable change in both the uncertainty and the relative uncertainty
 \item $100000 \pm 3900 \rightarrow (100 \pm 4)\times 10^3$ (4\% relative) - Change to scientific notation to easily see the size of the relative uncertainty, only use 1 digit on the uncertainty since changing between 3.9\% and 4\% relative uncertainty is not significant.
 \item $0.000084341387 \pm 0.0000003223 \rightarrow (84.3 \pm 0.3)\times 10^{-6}$ (0.4\% relative) - Scientific notation to make it more legible, only show 1 digit for the uncertainty since the relative uncertainty is already very small.
 \item $0.000091 \pm 0.1 \rightarrow 0.0 \pm 0.1$ (relative not defined) - The uncertainty being so much bigger than the value, we cannot distinguish it from zero, so we might as well quote it as zero. With a zero central value, we cannot determine the relative uncertainty. 
\end{itemize}

\end{example}

\section{Comparing measured quantities}
Now that we have some grasp on uncertainties, or at least an understanding that uncertainties are not easy to define accurately, how do we come to conclusions based on measured values? How do we conclude if two numbers with uncertainties agree? Again, we have to remember that we are scientists and that we work within the Scientific Method. We really need to think in terms of ``does a measurement exclude a hypothesis?''. When comparing two measurements, we must then ask ourselves if one measurement is ``incompatible'' with the other one.

Suppose that two teams have each measured the mass of a dark matter particle. Team 1 measured the mass to be $m_1$=(10.4 $\pm$ 1)\,GeV and Team 2 measured the mass to be $m_2$=(12.5 $\pm$ 1)\,GeV. Do the results agree, or should we conclude that there may in fact exist two different dark matter particles? Well, it depends! What is represented by the uncertainties? If both teams are betting their lives that the mass is definitely included within the range that they quote, then the measurements disagree (since the highest compatible value with Team 1 is 11.4 and the lowest value compatible with Team 2 is 11.5). In general however, results are reported with some ``degree of confidence'' that the true value lies within the specified range. Often, that degree of confidence is around 68\% (which we will motivate in chapter \ref{chap:StatsNormal}). If the two teams are both quoting there result with a 68\% confidence that the true value lies in their quoted range, there is some room for the value to be outside of the respective ranges and for the two measurements to agree (and indeed, we would conclude that the two measurements, at this level of precision, are consistent with each other).

We can also use the relative difference between the measurements and compare that to the relative uncertainty on the measurements to get an idea of the agreement in terms of the precision. Team 1 has a relative uncertainty on their measurement of $\frac{1.0}{10.4}=9.6\%\sim 10\%$, and Team 2 has a relative uncertainty in their measurement of $\frac{1.0}{12.5}=8\%$; the two measurements thus have similar precision, around 10\%. The ``relative difference'' between the central value of the two measurements is $\frac{12.5-10.4}{12.5} = 16.8\% \sim 17\%$. In this analysis, given that the relative difference between the measurements (17\%) is of the same order as the quadrature sum of the individual relative uncertainties ($\sqrt{(10\%)^2+(8\%)^2}\approx 13\%$), one would not conclude that the measurements are incompatible, even if the ranges covered by the uncertainties do not quite overlap.

As a scientific community, we would want to understand if both teams have correctly represented their uncertainties, we would want to see the experiments repeated to be more precise, and if the difference subsists, we would first assume that there is a systematic uncertainty in at least one of the experiments that is shifting their value one way. Only after all of those issues have been addressed, and if the difference between the two measurements was many times the uncertainty between them, we would consider accepting that there may be two different dark matter particles.

Again, we find that we have to be very precise when comparing results, as the conclusions that we draw are dependent on how the uncertainties are determined. Even worse, the standard that we apply to make a conclusion may well depend on how important that conclusion is. Let's say that instead of measuring the mass of a dark matter particle, Team 1 simply claims that they have finally discovered dark matter (which would certainly earn them a Nobel Prize). In their results, they report that they expected 5 $\pm$ 2 background events, and instead observed 11 events. They claim that the excess of 6 events over the expected background of 5 is a clear indication of dark matter events, as it is larger than the expected background by 3 times the uncertainty on the background rate. Indeed, a measurement of 11 is not very consistent with 5 $\pm$ 2, so the hypothesis that they observed an unusually large fluctuation in the background is difficult to support. We also assume that the community has thoroughly analysed their background estimate and agrees that 5 $\pm$ 2 is reasonable. As we will see in chapter \ref{chap:StatsNormal}, there is approximately a 0.3\% chance that the background actually did fluctuate by 3 times its uncertainty. Since we have to significantly change the laws of physics if the measurement is correct, we may not be satisfied with a 0.3\% chance that the conclusion of dark matter existing is wrong. And indeed, in particle physics, the standard is often that a measurement must disagree with the null hypothesis by 5 times its uncertainty in order to invalidate the null hypothesis (in this case, the null hypothesis is that Team 1 only observed background events).

\section{Summary}
Hopefully this chapter will have convinced you of the following points, which you should typically think about when discussing your results:
\begin{itemize}
\item There is no perfect rule to determine the size of uncertainties in a measurement; the goal is to make sure that the uncertainties are justified.
\item Overestimating uncertainties is just as bad as underestimating them. If you think your uncertainties are too small, understand why and look for systematic effects.
\item If possible, you should perform independent measurements to gauge whether your uncertainties are well estimated.
\item It is absolutely critical to present results precisely (explain how uncertainties were determined), otherwise it is impossible to use the results.
\item Systematic errors are the plague of experimental physics and the design of experiments should focus on minimizing them.
\item Random errors can be minimized by repeating measurements (but some measurements may not be easy to repeat).
\item When you quote a value with uncertainty, $x\pm\sigma_x$, you should mean that it is equally likely for the true value to be on either side of $x$.
\item Comparing the relative difference between values to the relative uncertainties on the values (or their sum in quadrature) gives a good idea of whether the values agree.
\item The ultimate comparison between numbers with uncertainties involves some degree of subjectivity and depends on: what the uncertainties are defined to be, whether systematic errors have been considered, and what standard needs to be met for a hypothesis to be rejected.
\end{itemize}

With the caveat that you need to consider systematic uncertainties and that this may underestimate your uncertainty, it is usually safe to use half of the smallest division from an instrument as the uncertainty on a direct measurement from the instrument. 

We saw that for a set of $N$ independent measurements, \{$x_1, x_2, x_3, \dots, x_n$\} of a quantity, $x$, the quoted result should be given as $\bar x \pm \frac{\sigma_{\bar x}}{\sqrt{N}}$, where
\begin{align}
\label{eqn:meanAndErrorOnMean}
\bar x &\equiv \frac{1}{N}\sum x_i \nonumber\\
\sigma_x &\equiv \sqrt{\frac{1}{N-1}\sum (x_i-\bar x)^2}\nonumber\\
\sigma_{\bar x} &\equiv \frac{\sigma_x}{\sqrt{N}}
\end{align}
$\sigma_x$ is called the standard deviation of the measurements and is representative of the uncertainty on a single measurement. $\bar x$ is the mean of the measurements, and $\frac{\sigma_x}{\sqrt{N}}$ is the uncertainty on the mean. If the measurements are independent and there are no systematic effects, there is a 68\% chance that the true value lies within the specified range.

In a counting experiment, where events occur at random times but with a well defined average rate, the uncertainty on the number that you counted, $N$, is simply the square root, $\sqrt{N}$. This uncertainty is constructed so that it is the standard deviation that you would get if you measured $N$ many times. If N is large (say bigger than about 80), the range given by $N\pm \sqrt{N}$ will have the true value in it 68\% of the time, if N is small, that percentage will be somewhat reduced.
